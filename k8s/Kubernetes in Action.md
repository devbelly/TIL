![image](https://github.com/devbelly/TIL/assets/67682840/48aacc66-5380-4e6e-89f5-058bacad0584)


# 1장. 쿠버네티스 소개

- 모놀리스 애플리케이션의 확장
	- 수직확장 : CPU, 메모리 등 하드웨어 장치의 업그레이드
		- 상한이 존재한다
	- 수평확장: 서버를 여러대 증설
		- 코드의 변경이 이루어져야한다
		- 항상 적용되는 것은 아니다.
- 마이크로서비스의 필요성이 나타나게 된다
	- 개발자가 잘 이해하고 있는 HTTP, AMQP 프로토콜을 통해 마이크로서비스간 통신을 한다
		- 최근에는 gRPC, RSocket을 활용한다
	- 하나의 구성요소가 변경되더라도 전체를 변경하지 않아도 된다
- 마이크로서비스의 단점
	- 규모가 커지면 각 구성요소들을 배포 및 관리하는 것이 어려워진다.
	- 분산된 환경에서 원격 호출하므로 디버깅 및 추적이 어렵다 → Zipkin으로 해결가능

## 1.2 컨테이너 기술 소개

- 하나의 호스트에서 각 애플리케이션마다 종속성 및 요구사항을 동시에 만족하는 것은 굉장히 어렵다
- 또한 개발자 및 운영자가 실행하는 애플리케이션 환경이 다르다
	- 마이크로서비스가 많지 않다면 VM을 통해서 운영체제 인스턴스를 제공 → 환경을 격리할 수 있다.
	- 서비스의 크기가 작아지고 갯수가 많아지면 일일이 VM을 통해서 환경을 격리한다면?
		- 하드웨어 리소스가 낭비된다
		- 일일이 관리해야하므로 인적자원이 낭비된다.
- VM 대신 리눅스 컨테이너 기술에 눈을 돌리게 되었다.
- VM와 리눅스 컨테이너
	- 차이점
		- VM은 애플리케이션이 가상화된 OS 커널에 대한 syscall을 호출 → 커널이 하이퍼바이저를 통해 물리적 CPU를 조작
		- 컨테이너는 호스트 OS와 동일한 커널에서 syscall 호출
	- 특징
		- VM은 서로 다른 커널을 보유하므로 완전한 격리를 제공한다.
		- 컨테이너는 동일한 커널을 사용하므로 보안상 완전한 격리를 제공하지 못한다
		- 실행되는 프로세스의 수가 적으면 VM, 많다면 컨테이너를 고려하자
- 리눅스 컨테이너의 기반 기술
	- namespace : 각 커널 리소스마다 가상화된 환경을 제공하여 리소스간 분리를 가능케 한다
	- cgroup : 얼마나 자원을 할당할 지 결정할 수 있다.
- 리눅스 컨테이너 기술을 쉽게 사용할 수 있도록 하는 기술이 도커
	- 애플리케이션 패키징 배포 관리 기능을 제공한다.
- 도커 컨테이너 이미지 vs VM 이미지 차이
	- 도커 컨테이너는 레이어로 이루어져 있어 다룬 이미지를 다운할 때 동일한 레이어가 있다면 나머지 레이어만 다운

#### 컨테이너에서 동일한 파일 시스템을 공유하는 방법

- 도커 이미지는 레이어로 구성되어있다.
- 레이어는 읽기 전용 레이어 + 쓰기 전용 레이어로 나뉜다. 
	- 쓰기 전용 레이어는 컨테이너 레이어이며 컨테이너가 종료될때 같이 삭제된다
- 두 레이어를 동일한 파일 시스템으로 인식한다 → 이를 Union File System이라 한다

<img width="850" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/615c0547-6e9c-4cca-9769-63ac475e74a0">
- 컨테이너는 최종적으로 `merged`를 바라보며 만약 file2에 쓰기작업이 일어나면 upperdir에 반영된다. 
- 즉, 각 컨테이너는 독립적인 파일 시스템을 유지(merged)하면서도 동일한 파일 시스템(lowerdir)을 사용한다

## 1.3 쿠버네티스 소개

### 1.3.3 쿠버네티스 클러스터 아키텍쳐 이해

- 마스터 노드 + 워커 노드로 이루어져 있다.
- 컨트롤 플레인
	- 컨트롤 플레인은 여러 구성요소로 이루어져 있다.
		- API server
			- 쿠버네티스의 프론트엔드 역할
			- 사용자와 컨트롤 플레인 구성요소와 통신한다
		- etcd(엣시디)
			- 컨트롤 플레인의 데이터 저장소
			- 설정값, 클러스터 상태를 키-값 형태로 저장한다.
		- 스케줄러
			- 애플리케이션 배포를 담당
		- 컨트롤러 매니저
			- 워커 노두 추척, 장애처리 담당
- 워커 노드
	- kubelet
	- docker engine
	- kube-proxy

#### 쿠버네티스에서 애플리케이션 실행

- 과정
	- 애플리케이션을 이미지로 패키징
	- 이미지를 레지스트리에 저장 (도커 저장소, 구글 저장소 등)
	- 쿠버네티스 API 서버에 애플리케이션 디스크립터 게시(.yml)
	- 스케줄러가 노드 정보를 확인해 노드에 컨테이너 할당 지시
	- 각 노드에 있는 kubelet이 엔진(도커)에게 이미지 저장소에서 이미지를 가져오라고 지시
	- 가져온 이미지를 컨테이너로 실행

> [!info] 컨테이너는 OS를 포함할까?
> - 기본적으로 VM과 컨테이너 차이는 OS 포함 유무
> - 하지만 컨테이너로 CentOS를 사용하는 건 뭘까?
> 	- 이는 OS 범주를 어디까지 보느냐에 달린 문제
> 	- Host OS와 커널을 공유하는 컨테이너. 커널은 OS의 핵심 기능
> 	- OS의 범주를 확장하면 커널뿐만 아니라 다른 기능들도 많음 → 이 부분을 이미지로 얻어오는 것!

#### 이동한 애플리케이션에 접근

- 쿠버네티스가 제공하는 서비스를 통해 고정 IP에 접근
- 서비스는 동적인 컨테이너 IP를 관리한다

# 2장. 도커와 쿠버네티스 첫 걸음

## 2.1 도커를 사용한 컨테이너 이미지 생성, 실행, 공유하기

- 도커가 설치되어있지 않다면 도커를 설치하자
	- 리눅스 머신에서 도커를 설치해야한다
	- 만일 리눅스가 아닌 맥, 윈도우 OS라면 리눅스 가상머신이 설치되고 그 위에 도커 데몬이 실행된다

- Dockerfile 작성
	- `app.js` 파일을 작성했으면 같은 디렉토리에 이미지 패키징을 위해 Dockerfile을 작성해보자

```Dockerfile
FROM node:7
ADD app.js /app.js
ENTRYPOINT ["node", "app.js"]
```

- From : 기본 이미지를 지정한다. 노드 애플리케이션이므로 노드 이미지를 가져와 사용한다.
- ADD : 현재 디렉토리에 있는 `app.js`를 이미지의 루트 디렉터리에 `app.js`로 추가한다
- ENTRYPOINT : 이미지를 수행했을 때 실행되어야할 명령어를 의미한다

#### 어떻게 이미지가 빌드되는지 이해하기

- `docker build -t kubia .`
	- `-t` : 빌드할 이미지의 태그 이름을 지정한다.(kubia)
	- `.` : 현재 디렉터리의 `Dockerfile`을 사용한다.
- 이미지는 여러개의 레이어로 구성되어있다.
	- 이미지가 다시 빌드될 때 레이어를 캐싱해서 사용한다.

### 컨테이너 이미지 실행

- `docker run --name kubia-container -p 8080:8080 -d kubia`
	- `-p 8080:8080` : 호스트의 8080포트를 컨테이너 8080포트와 연결한다.
		- 대부분 경우 크게 문제되진 않으나 컨테이너의 특정 포트로 접근해야하는 경우 (MySQL 3306) 문제가 된다

## 2.2 쿠버네티스 클러스터 설치

### 2.2.1 Minikube를 활용한 단일 노드 쿠버네티스 클러스터 실행하기

- Minikube는 쿠버네티스를 테스트 & 애플리케이션 개발목적으로 사용

> [!info] Mac + podman 환경에서 minikube 사용하기
> - [Using Minikube on M1 Macs](https://itnext.io/using-minikube-on-m1-macs-416da593ba0c)


**클러스터 동작 상태 확인**
- `kubectl get nodes`

**파드 이해**
- 쿠버네티스는 컨테이너를 직접 다루지 않고 다수의 컨테이너를 한거번에 다룬다 → 이를 파드라고 한다
- 가장 기본적인 단위이며 한 개 이상의 컨테이너를 포함할 수 있다.
- Pod내의 컨테이너는 IP와 Port를 공유 + 디스크 볼륨을 공유할 수 있다.

**파드 조회하기**
- 컨테이너는 쿠버네티스의 독립적인 오브젝트가 아니다
- `kubectl get pods`

**백그라운드에서 일어난 동작 이해하기**
- `docker push devbelly/kubia`
	- 도커 데몬이 다른 머신에 존재한다면 이미지를 가져오기 위해 레지스트리를 사용해야하므로 저장소에 이미지를 올려둔다
- `kubectl run kubia --image=luksa/kubia --port=8080`
	- `kubectl`에게 명령을 전달
	- `kubectl`은 마스터 노드의 REST API 서버로 명령 전달 + 클러스터에 Replication controller 생성됨
	- 스케줄러가 파드를 워커노드에 스케줄링
	- 해당 노드에 있는 `kubelet`은 이미지를 가져와서 컨테이너를 실행한다

> [!info] Replication Controller?
> - 생성된 Pod를 지속적으로 관리하기 위한 Object이다.
> - `replicas` : 지정된 숫자로 Pod를 유지한다
> - `selector` : 지정된 라벨의 Pod를 관리한다
> - `template` : 추가로 Pod를 기동할 때 사용하는 정보들을 기술한다

> [!info] 클러스터에 Replication controller이 생성 ?
> - `kubectl run kubia --image=devbelly/kubia --port=8080 --generator=run/v1`
> - 위 명령어에서 `--generator=run/v1`가 레플리케이션 컨트롤러를 생성
> - 현재는 deprecated되어 해당 flag를 제외하면 `pod/kubia create`라고 콘솔에 뜨는 것을 확인할 수 있다.
>   <img width="455" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/f31f07ac-db47-4d31-a10a-87dcbfc9cd65">

### 2.3.2 웹 애플리케이션에 접근하기

- 각 파드는 자체 IP를 가지고 있지만 클러스터 내부에 존재하므로 외부에서 접근하기 어렵다.
- 서비스를 생성해도 클러스터 내부에서 접속가능하므로 LoadBalancer 타입의 서비스를 생성해야한다
- `kubectl expose` : 서비스를 생성하여 리소스를 외부로 노출하는 역할을 한다.

**서비스 오브젝트 생성하기**
- `kubectl expose rc kubia --type=LoadBalancer --name kubia-http`
- 위에서 `replication controller`를 생성하지 않았으므로 파드를 외부로 노출하자

**파드와 컨테이너의 이해**
- 파드는 여러 개의 컨테이너를 가질 수 있다.
- 파드는 고유한 사설 IP 주소와 호스트 이름을 갖는다 → `app.js`의 `os.hostname`을 호출해도 워커노드 대신 고유한 호스트 이름이 출력됨을 확인할 수 있다. (교재에서는 kubia-4jfyf)
- 파드에 대한 자세한 정보를 알고 싶다면 `kubectl get pods -o wide` 를 사용할 수 있다.
	- 파드가 어떤 노드에 스케줄링 되어있는지 확인할 수 있다!

**레플리케이션컨트롤러의 역할 이해**
- `replicas`로 지정한 숫자만큼 pod를 띄우도록 돕는다

**서비스가 필요한 이유**
- 파드는 일시적이다
- 파드는 불완전한 노드가 삭제되면 같이 삭제될 수 있고 누군가에 의해 없이질 수도 있다.
- 레플리케이션 컨트롤러가 이를 제어해 파드를 `replicas`만큼 유지하도록 다시 생성한다.
- 이때 생성되는 사설 IP 주소는 계속해서 변경
	→ 이를 관리하기 위해 서비스가 필요하다!

# 3장. 파드 : 쿠버네티스에서 컨테이너 실행

**왜 쿠버네티스는 여러 컨테이너를 관리하는 파드라는 단위가 필요할까?**
- 선행지식) 컨테이너는 왜 단일 프로세스를 지향할까?
	- 하나의 컨테이너에서 여러 프로세스를 띄우면
		- 개별 프로세스의 추적 및 재시작을 직접 관리해야한다.
		- 표준 출력으로 인해 로깅 추적이 어렵다 → 무슨말인지…?
- 위 이유로 컨테이너를 관리하는 상위 개념인 파드를 도입한다.
	- 동시에 실행되어야 하는 컨테이너를 관리하면서도 각 컨테이너의 독립성을 유지할 수 있다.

**파드의 네임스페이스**
- 기본적으로 네임스페이스는 파드 단위로 공유되지만
- 파일 시스템에 한해서는 컨테이너 개별적으로 가지고 있다. → 쿠버네티스의 볼륨 시스템으로 파일 디렉터리를 공유할 수도 있다

**컨테이너가 동일한 IP와 포트 공간을 공유하는 방법**
- 네임스페이스는 여러가지가 존재한다. 그 중 네트워크 네임스페이스라는 것이 존재한다.
- 같은 파드끼리는 동일한 네트워크 네임스페이스를 공유한다
	→ 같은 IP와 Port 공간을 공유
- 위 이유로 동일한 파드 내 컨테이너 끼리는 `localhost:~`로 서로 통신이 가능
- 포트가 겹치지 않도록 주의해야한다!

**컨테이너 포트 지정**
- `yaml` 디스크립터를 통해서 Pod를 지정할 수 있다.
- `spec`에 정의된 `containerPort`는 다른 개발자에게 정보를 제공하기 위한 목적이다
- 공식 문서에서 그 내용을 확인할 수 있다
- 또한 포트를 명시적으로 정의하면 이름을 지정하게 편리하게 사용할 수 있다

	> List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default "0.0.0.0" address inside a container will be accessible from the network. Cannot be updated.

**kubectl create 명령으로 파드 만들기**
- `kubectl create -f kubia-manual.yaml`
- `yaml`, `json`으로 작성되었다면 위 명령어로 파드  생성 가능

**출력 로그 확인하기**
- 컨테이너화된 애플리케이션은 파일 대신 표준출력 또는 표준에러를 사용한다
- `kubectl logs <pod-name>`
- 하나의 파드에 여러 컨테이너가 존재한다면 `kubectl logs -c <container-name>`
- 파드가 삭제되면 로그도 삭제됨. 원치 않다면 중앙집중식 로그를 사용해야한다

### 3.2.5 파드에 실제 요청 보내기
- 2장에서 생성된 파드의 실제 동작을 확인하기 위해 `EXPOSE`를 통해 서비스를 생성
- 간단한 디버깅 목적을 위해서 서비스를 띄우는 대신 포트 포워딩을 사용하자
	<img width="389" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/1b6ef23b-43fa-4efe-8001-f106dc94dff8">

## 3.4 레이블 셀럭터를 이용한 파드 부분 집합 나열

- 쿠버네티스의 실제 운영환경은 여러개의 파드(수십~수백)로 이루어져있다.
- 여러 파드를 관리하기 위해 `키-값`으로 구성된 레이블을 사용
- 레이블을 추가하는 방법
	- `yaml`에 작성
	- 명령어를 통해 추가, 수정(수정시 `--overwrite` 플래그가 필요하다)
- 레이블은 파드만이 가질 수 있는 것이 아니다! 쿠버네티스 오브젝트라면 가질 수 있다.
- 만약 특정 노드에 파드를 스케줄링 해야한다면? (인프라에 대한 의존도가 생기긴 한다)
	- 노드에 `gpu=true` 레이블을 추가한 후
	- 파드에 `nodeSelector` 설정에 `gpu: "true"` 를 적는다.

## 3.7 네임스페이스를 사용한 리소스 그룹화

- 레이블을 이용해 다른 오브젝트를 묶을 수 있지만 각  오브젝트는 여러 여러 레이블을 갖기 때문에 교집합이 되는 오브젝트가 생긴다
- 겹치지 않는 리소스로 구분하기 위해서는 네임스페이스 사용
	- 이는 리눅스 네임스페이스와는 다른 개념

> [!faq] 쿠버네티스 네임스페이스는 오브젝트 이름의 범위를 제공한다
> - 리소스 이름은 네임스페이스 내에서 유일
> - 네임스페이스를 통틀어서 유일할 필요는 없다.
> - 여러 개발자들이 자신의 네임스페이스에서 작업한 것이 다른 사람의 네임스페이스에 영향을 끼치지 않도록 한다


- `kubectl get ns`으로 가지고 있는 네임스페이스 목록을 출력할 수 있다
- `kubectl get po`는 ns를 지정하지 않았기 때문에 기본적으로 `default namespace`에 작업을 한다

> [!faq] 오브젝트와 리소스
> - 오브젝트 : 구체적인 객체를 가리킨다. 
> - 리소스 : API endpoint를 가리킨다. 대부분 오브젝트를 가리키나(GET) 간혹 동작을 가리키기도 한다(POST)
> - 리소스와 오브젝트는 클래스와 객체의 관계와 일치한다.
> 	- `kubectl get po`했을때 결과가 `apple`, `banana` 라면 
> 	- `apple`, `banana`는 오브젝트
> 	- `po`는 리소스


- 네임스페이스에 리소스를 만들기 위해서는 `namespace: custom-namespace`를 지정하거나 `kubectl create`를 사용한다

# 4장. 레플리케이션과 그 밖의 컨트롤러: 관리되는 파드 배포

## 4.1 파드를 안정적으로 유지하기

- 컨테이너 리스트를 쿠버네티스에 전달하면 쿠버네티스는 파드가 사라지지 않는 한 컨테이너를 계속 관리한다
- 만약 컨테이너의 주 프로세스가 예기치 못한 이유로 강제종료되면 쿠버네티스는 컨테이너를 다시 실행한다
- 강제종료가 된 상황이 아니라 컨테이너의 프로세스가 무한루프 또는 데드락 같이 응답을 제대로 못 보내는 상황이면 어떡할까?
	- 이러한 상황도 쿠버네티스가 관리하기 위해서 애플리케이션 내부적으로 상태를 추적하는 것 뿐만 아니라 애플리케이션 외부에서도 컨테이너 상태를 계속 확인할 수 있어야 한다
	- 이를 위해서 liveness probe를 사용!

**liveness probe의 세가지 매커니즘**
- Http Get
	- 지정된 IP, port, 주소를 기반으로 http get 메세지를 날려서 응답하는 코드에 따라 컨테이너를 재실행한다
- TCP socekt
	- 지정된 port를 기반으로 TCP 연결을 시도한다
- Exec 프로브
	- 컨테이너에 임의의 명령을 실행한 후 리턴되는 값을 확인한다

<img width="445" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/0ff1d964-d9a3-4478-a41c-eaa7985da4ce">
- 일정시간마다 Http Get요청을 보내고 실패하여 RESTARTS가 1로 증가한 것을 확인할 수 있다.
- `kubectl describe po liveness-probe`를 통해 종료된 이유를 확인할 수 있다.
	- 종료코드는 137로 128+x를 의미
	- 외부 신호에 의해 컨테이너가 종료되었다는 것을 확인. x는 9 이므로 강제종료되었다는 것을 알 수 있다.
- 컨테이너가 비정상적으로 작동 시 노드의 `kubelet`이 컨테이너를 재실행할 책임을 지며
	- 노드가 비정상적으로 종료되면 마스터 노드의 `control-plane`이 파드를 재실행하게 된다

## 4.2 레플리케이션 컨트롤러

- 파드의 갯수가 일정하게 유지되도록 하는 리소스
	- 노드가 비정상적으로 종료되어 파드가 사라지거나
	- 클러스터에서 노드가 사라졌을때
- 레이블 셀렉터와 매치되는 파드를 찾아 항상 일정수를 유지하고 있는지 모니터링한다
- 구성요소
	- 레이블 셀렉터
	- 템플릿
	- 레플리카
- 레이블 셀렉터와 템플릿의 변경은 현재 레플리케이션컨트롤러가 관리하고 있는 파드에 영향을 끼치지 않는다.
- 레이블 셀렉터와 템플릿의 파드 레이블은 완전히 일치해야한다
	- 일치하지 않으면 파드를 무한정 생성, 이를 방지하기 위해 미리 검사하기는 한다
	- 레이블 셀렉터를 지정하지 않고 템플릿의 파드 레이블을 자동으로 찾게끔 작성하자
		- `yaml`를 가볍게 유지할 수 있고 실수를 줄일 수 있다.


	<img width="473" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/3cb1c9fa-e55a-45b0-b39c-ba6abe804fec">
- 생성 직후

<img width="457" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/ba21bb4d-aae9-42d9-8c1e-45ea16597204">
- 임의로 파드를 삭제하더라도 ReplicationController설정에 따라 자동으로 재시작함을 알 수 있다.

> [!info] 파드는 레플리케이션컨트롤러와 묶여있지 않다.
> - 레플리케이션컨트롤러는 레이블 셀렉터로 설정한 파드만 관리한다
> - 파드는 `metadata.ownerReference` 필드를 참조하여 레플리케이션컨트롤러를 쉽게 찾을 수 있다

> [!faq] 레플리케이션컨트롤러로 생성한 파드의 레이블을 변경하면 수동으로 생성한 파드와 동일해지는데 `metadata.ownerReference`는 어떻게 변할까?

`레이블 변경 전`
<img width="1103" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/d6da67ca-1a2a-432c-8f25-6706687bd402">
`레이블 변경 후`
<img width="641" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/f6d610cd-0ca4-4b15-9631-2c24c776ed49">
- 더이상 정보가 표시되지 않는다

- 레플리케이션컨트롤러가 삭제되면 관리되는 파드도 기본적으로 삭제되는 정책을 가진다.
	- 관리되는 파드를 남기고 싶을 수도 있다.(예를 들어 레플리케이션컨트롤러를 레플리케이션셋으로 변경하는 경우)
	- `--cascade=false`로 설정가능하다

### 4.3.2 레플리카셋 정의하기

- 레플리카셋은 레플리케이션컨트롤러와 거의 유사하나 레이블을 선택할 때 다양한 표현식을 사용할 수 있다
- 레플리케이션 컨트롤러에서 버린 파드를 주워서 사용해보자
- `kubectl describe rs`를 하면 `Event`가 없는 것을 알 수 있다. → 주워서 사용중

## 4.4 데몬셋

- 지금까지 살펴본 레플리케이션컨트롤러 & 레플리카셋은 클러스터에 지정한 파드만큼 무작위로 배포
- 하지만 노드마다 하나의 파드만이 실행되기를 원할수도 있다
	- 시스템 관련 파드들이 이에 해당한다. 모니터링
- 이럴때 데몬셋을 활용할 수 있고 모든 노드마다 파드를 실행하기를 원하지 않는다면 `nodeSelector`를 통해 특정 노드에서만 파드를 실행할 수 있다
- 데몬셋의 개념 때문에 `replicas` 개념이 없다

## 4.5 잡

- 지금까지 살펴본 레플리케이션컨트롤러 & 레플리카셋& 데몬셋은 항상 컨테이너가 실행되어야한다는 공통점이 있다.
- 하지만 컨테이너가 태스크를 완료했다면 컨테이너를 다시 시작하지 않고 종료되기를 원한다면 어떡할까? → 잡 리소스를 사용할 수 있다
- 노드가 비정상적으로 종료되면 파드는 다른 노드에 자동으로 스케줄링된다
- 프로세스가 비정상적으로 종료되면 잡에서 다시 시작할지에 대한 유무를 선택할 수 있다.
- `kubectl get job`을 해보면 숫자로 표시된다
	- 이는 `completions`, `parallelism` 옵션이 더 있음을 예상할 수 있다.

## 4.6 크론잡

- 잡을 주기적으로 실행하려면 크론잡 리소스를 사용할 수 있다.
- 쿠버네티스는 설정된 시간에 크론잡 오브젝트에 설정한 잡 템플릿에 따라 생성한다
- 잡 템플릿은 설정한 파드 템플릿에 따라 파드를 생성하고 실행한 후 종료된다

# 5장. 서비스

- 쿠버네티스는 파드가 계속 생성되고 삭제되는 일이 잦다.
- 위 이유로 파드의 IP주소는 계속 변경되어 클라이언트에서는 고정된 IP에 접근하기 어렵다
- 위 문제를 해결하기 위해 서비스 리소스를 이용할 수 있다
- 이전에 `kubectl expose`를 통해 간단하게 서비스를 만들었지만 `yaml` 로 작성하는 방법에 대해 알아보자

```yaml
apiVersion: v1

kind: Service

metadata:

name: kubia

spec:

ports:

- port: 80

targetPort: 8080

selector:

app: kubia
```

	- 서비스의  포트는 80이다
	- 서비스가 포워드할 컨테이너 포트는 8080이다 -> 이미지 생성시 사용한 포트
	- `app: kubia`를 가지고 있는 파드를 서비스로 묶는다

- `kubectl get svc`를 통해 현재 실행중인 서비스를 확인할 수 있다.
	- 기본 목적은 파드의 그룹화지만 주로 파드를 외부 노출하기 위해 사용한다.
	- 현재는 클러스터 내부 IP가 할당된 것을 알 수 있다

<img width="476" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/92f4f35f-adbb-4e2c-9a0c-98a0ed13d6f3">
- 내부 컨테이너에 접속해서 서비스의 내부 IP에 `curl` 요청을 보내고 있다
- 서비스 프록시가 요청을 랜덤한 파드로 보내기 때문에 실행마다 결과값이 변하고 있다
- 클라이언트 IP마다 처리할 파드를 고정하기 위해서는 `sessionAffinity` 옵션을 추가하면 된다

**이름이 지정된 포트 사용**
- 서비스의 타켓포트를 번호로 지정하는 대신 이름을 사용할 수 있다
- 파드 생성시 컨테이너포트에 이름을 지정한 후 서비스에서 이 이름을 사용할 수 있다
	- 이전에 파드에서 컨테이너포트를 지정한 것은 개발자를 위한 것이라고 했다
	- 뿐만 아니라 이름을 지정하여 편리하게 사용할 수 있다고 했는데 지금 이 부분이다

**환경변수를 통한 서비스 검색**
- 생성한 서비스를 클라이언트 파드에게 일일이 전달하는 대신 검색할 수 있는 기능을 제공한다
- 파드가 생성되면 해당 시점에 존재하는 서비스에 대한 환경변수를 파드에 저장하게 된다
	- 즉 파드생성 → 서비스생성시 서비스를 제대로 찾지 못한다
- `kubectl exec <pod> env`

 **DNS를 통한 서비스 검색**
 - `kube-system` 네임스페이스에는 `kube-dns`이름의 서비스가 있다
	<img width="797" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/d0f55608-c88f-4521-99b4-96a596d6b10f">
- 모든 파드에서 이뤄지는 dns 질의는 `kube-dns`를 사용하도록 설정되어있다
	- 이는 리눅스 시스템의 `/etc/resolve.conf`를 확인해보면 알 수 있다
	- <img width="611" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/1dbd0bab-45ca-4ec3-8fe2-4a8185246884">

### 5.2.1 서비스 엔드포인트
- 서비스는 파드에 직접 연결되지 않는다
- 서비스와 파드 사이에 엔드포인드가 있다.(엔드포인트도 리소스이다)
- 파드 셀렉터는 엔드포인트 목록을 만드는데 사용된다

```yaml
apiVersion: v1
kind: Service
metadata:
	name: external-service
spec:
	ports:
	- port: 80
```
	 
### 5.2.3 외부 서비스를 위한 별칭 생성

- 엔드포인트를 직접 구성하는 대신 FQDN을 통해 외부 서비스를 참조할 수 있다.
- `externalName.` 클러스터 내부 파드가 외부 서비스에 쉽게 접근하기 위해 사용한다
- CNAME DNS record를 생성하여 coreDNS에 해당 정보를 저장한다

## 5.3 외부 클라이언트에 서비스 노출

- 내부→외부로 접근하는 `externalName` 또는 수동으로 엔드포인트 구성
- 외부→내부로 접근하려면 어떻게 해야할까?

### 5.3.1 노트포트 서비스 사용

- 기본방식인 클러스터의 IP 접근 + (노드 IP+노트 Port)로 추가접근이 가능
- 접근
	- `10.111.254.223:80`
	- `<첫 번째 노드의 IP>:30123`
	- `<두 번째 노드의 IP>:30123`

### 5.3.2 외부 로드밸런서로 서비스 노출

- 클라이언트는 접근하는 노드에 상관없이 서비스에 접근할 수 있어야한다
	- 하지만 노트포트로 접근하면 접근하는 노드의 상태에 따라 서비스에 접근하지 못하는 경우도 발생한다
- 로드밸랜서를 사용해서 해결하자
- `yaml` 파일 작성 시 `nodePort`를 지정할 수 있지만 작성하지 않는다 → 쿠버네티스가 직접 선택하도록 한다

### 5.3.3 외부 연결 특성 이해

- 불필요한 hop
	- 노트포트로 노드 접근 → 서비스 접근 → 파드 접근
	- 만약 노드에 처음 접근했을 때 파드가 실행된다면 불필요한 홉이 발생한다
	- `externalTrafficPolicy`
		- 외부 트래픽에 대한 정책을 결정한다.
			- Local 사용 시 불필요한 홉을 막을 수 있으나 트래픽 불균형을 가져온다

- 클라이언트 IP 변경
	- 노트포트 사용 시 SNAT 때문에 클라이언트 IP가 변경된다
	- `externalTrafficPolicy`를 Local로 하면 변경되는 것을 막을 수 있다
	- [여기](https://kubernetes.io/ko/docs/tutorials/services/source-ip/) 를 참고하자..

## 5.4 인그레스

- 서비스는 L4 계층에서 작동. L7 계층에서 로드밸런싱을 지원하기 위해 인그레스를 사용할 수 있다
	- 이는 도메인 이름 마다 또는 경로마다 로드밸런싱을 지원할 수 있따
- 사용하기 위해서는 인그레스 컨트롤러를 설치해야한다
- TLS도 지원할 수 있다. TLS termination 기능 또한 지원
	- TLS termination을 통해 성능 향상을 기대할 수 있다

## 5.5 레디니스 프로브

- 새로운 파드가 추가될 때 온전한 요청을 수신하기까지 시간이 필요할 수도 있다 → 레디니스 프로브를 통해 이를 확인할 수 있다.
- 라이브니스 프로브와 마찬가지로 레디니스 프로브도 세가지 유형이 존재한다
	- exec
	- Http Get
	- TCP 소켓
- 주기적으로 프로브를 호출해 상태를 점검한다. 이때 만약 준비되지 않았다면 서비스의 엔드포인트에서 제거되었다가 준비되면 다시 엔드포인트에 추가된다
	- 서비스의 엔드포인트는 `kubectl describe svc <service-name>` 에서 확인한다

> [!faq] p252. 레디니스 프로브에 파드의 종료 코드를 포함하지 마라

# 6장. 볼륨

- 파드는 내부에 프로세스가 실행되고 CPU, RAM, 네트워크 인터페이스 등의 리소스를 공유하는 논리적인 호스트와 유사하다 → 파드 내부의 컨테이너끼리는 `localhost:port`로 통신하기 때문이다
- 프로세스가 디스크를 공유하듯 컨테이너끼리 디스크를 공유할 수 있을까? 
	- 파드 내부의 컨테이너는 독립적인 파일 시스템을 가진다
	- UFS를 사용한다. 책 설명대로 파일 시스템은 컨테이너 이미지에서 제공!
- 📖 새로 시작한 컨테이너는 이전에 실행했던 컨테이너에 쓰여진 파일 시스템의 어떤 것도 볼 수 없다
	- 컨테이너에서 사용하는 파일 시스템은 UFS
	- UFS는 `image layer` + `container layer`가 합쳐진 형태
	- 컨테이너가 생성되며 생기는 `container layer`는 컨테이너 종료시 사라지므로 이전에 실행했던 내용을 볼 수 없는 것!

## 6.2 볼륨을 사용한 컨테이너 간 데이터 공유

- 볼륨은 파드와 라이프 사이클을 공유하는 것이 일반적이다
- 볼륨의 유형 중 `emptyDir`에 대해 알아보자
- `emtpyDir`을 사용하는 경우
	- 하나의 파드에서 여러 컨테이너간 데이터를 공유할 때
	- 하나의 컨테이너에서 정렬 작업을 수행할 때 사용하는 메모리 공간이 부족할 때
	- 컨테이너에 `read/write layer`  가 존재하지만 `write layer`에 쓰는 것이 불가능한 상황일 때

## 6.3 워커 노드 파일시스템의 파일 접근

- 파드는 호스트 노드의 파일 시스템을 인식하지 못한다
- 파드중 특별한 파드(데몬셋)는 노드의 파일 시스템에 접근해야하는 경우가 있다.
- 이때 `hostPath`유형의 볼륨을 사용할 수 있다.
- 퍼시스턴트 스토리지로 `emptyDir`와 다르게 파드가 삭제되더라도 데이터가 남아있다
	- 파드가 재시작될때 동일한 호스트 노드에서 재시작 되어야 기존 데이터를 다시 볼 수 있따
	- 이러한 이유로 데이터베이스의 용도로 사용하지 않는다
	- 호스트 노드의 로그, 구성파일, CA 인증서 접근 용도로 사용된다

# 7장. 컨피그맵과 시크릿

## 7.1 컨테이너화된 애플리케이션 설정

- 컨테이너에서 설정 데이터를 전달할 때 환경 변수를 주로 사용한다
	- 명령어 인자로 전달하거나 파일로 전달하는 것은 애플리케이션에서 하드코딩한 것과 비슷하다

## 7.2 컨테이너에 명령줄 인자 전달

- 도커파일에서…
- `ENTRYPOINT`는 실행할 명령어를 작성한다.
	- `shell` 방식 : ENTRYPOINT node app.js
	- `exec` 방식 : ENTRYPOINT [“node”,“app.js”]
	- 노드를 실행하는데 쉘을 굳이 실행할 필요는 없으므로 `exec` 방식을 사용하다
- `CMD`는 ENTRYPOINT에 전달되는 매개변수를 지정한다
- `fortuneloop.sh`

	```bash
	...
	INTERVAL=$1
	...
	```

### 7.3.3 하드코딩된 환경변수의 단점

 - 파드정의를 재활용하지 못하고 환경마다 `yaml` 파일을 가지고 있어야한다
 - 컨피그맵으로 이를 해결할 수 있따

## 7.4 컨피그맵

- 키와 값 쌍으로 이루어진 데이터를 저장하는데 사용
- 기밀이 아닌 데이터를 저장한다 → 기밀이 필요하다면 시크릿을 사용
- 파드와 컨테이너는 동일한 네임스페이스에 존재해야한다
- 📖컨피그맵은 짧은 문자열에서 전체 설정 파일에 이르는 값을 가지는 키/값 쌍으로 구성된 맵이다

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: game-demo
data:
  # 속성과 비슷한 키; 각 키는 간단한 값으로 매핑됨
  player_initial_lives: "3"
  ui_properties_file_name: "user-interface.properties"

  # 파일과 비슷한 키
  game.properties: |
    enemy.types=aliens,monsters
    player.maximum-lives=5    
  user-interface.properties: |
    color.good=purple
    color.bad=yellow
    allow.textmode=true    
```

- 📖 애플리케이션은 필요한 경우 코버네티스 REST API 엔드포인트를 통해 직접 읽을 수 있지만 반드시 필요한 경우가 아니라면 자제하자
	- 컨피그맵을 사용하는 방법은 다음과 같다
		- 컨테이너 커맨드 & 인수
		- 컨테이너 환경변수
		- 읽기 전용 볼륨
		- 쿠버네티스 API
	- 쿠버네티스 API를 사용하는 방법은 다른 네임스페이스의 컨피그맵에도 접근할 수 있다

**환경변수를 컨피그맵에서 가져오는 예시**

<img width="324" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/44082e1e-a7dd-4540-8f89-5ae145969849">
- name, value 대신 name, valueFrom을 사용하는 것을 확인할 수 있다
- 환경변수를 일일이 가져오는 대신 한번에 가져오는 옵션인 `envFrom`은 쿠버네티스 1.6부터 추가되었다.

**컨피그맵 볼륨을 마운트한 디렉토리를 살펴보는 명령어**

- `kubectl exec kube-configmap-volume -c web-server ls /etc/nginx/conf.d`

**디렉터리를 마운트할 때 디렉터리의 기존 파일을 숨기는 것의 이해**
- 마운트하기 전(컨테이너면 fortune2s는 신경쓰지 말자)
	<img width="569" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/2544d935-31ce-4a39-b43f-1e83f523ce4f">
- 마운트한 후
<img width="902" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/29a7a75a-4dd2-41b3-a961-fa8f53a079cf">
- 기존파일이 숨겨지고 마운트한 볼륨의 내용만 접근가능함을 알 수 있다

**디렉터리 안에 파일을 숨기지 않고 개별 컨피그맵 항목을 파일로 마운트**
- `subPath`를 사용하자
- 하지만 업데이트와 관련해 큰 문제

**애플리케이션을 재시작하지 않고 애플리케이션 설정 업데이트**
- 환경변수나 명령어 인수로 값을 전달하면 업데이트를 위해 파드나 컨테이너를 재시작해야한다
- 컨피그맵을 업데이트하면 이를 참조하는 모든 볼륨의 파일이 업데이트된다

**파일이 한꺼번에 업데이트되는 방법 이해하기**
- 위 사진(컨피그맵 볼륨)에서 디렉터리 내용을 보면 심볼릭 링크 사용
- 즉 컨피그맵을 업데이트하면
	- `2023_07-03_...`와 같은 형식의 디렉터리가 새로 생성된다
	- 컨피그맵 볼륨에 있는 파일들(심볼릭링크)는 새로운 디렉터리를 가리킨다
	- 즉 모든 파일이 한번에 업데이트 된다

## 7.5 시크릿으로 민감한 데이터를 컨테이너로 전달

- 시크릿이 가지고 있는 `ca.crt`, `namespace`, `token`은 파드가 쿠버네티스 API와 통신할 때 사용한다

# 8장. 애플리케이션에서 메타데이터에 엑세스하기

> [!info] container resource vs limit
> - resource는 파드가 노드에 스케쥴링되는 기준을 정한다(Round Robin을 사용한다)
> - limit은 파드가 사용할 수 있는 최대 사용량을 의미한다

## 8.1 Downward API로 정보전달하기

- 파드나 컨테이너의 메타데이터는 환경변수 or Downward API 볼륨을 사용해 컨테이너에 전달할 수 있다.
	- 파드 및 컨테이너 필드를 노출하는 이 두가지 방식을 Downward API라고 한다
- 환경변수를 사용하는 경우는 파드의 레이블 또는 어노테이션을 전달하지 못한다.
- 환경변수로 메타데이터를 전달하는 방법

```yaml
...
spec:
	containers:
		env:
		- name: POD_NAME
		  valueFrom:
			  fieldRef:
				  fieldPath: metadata.namespace
...
```

- Downward API 볼륨으로 메타데이터를 전달하는 방법

<img width="490" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/b2669e18-8255-44dc-990b-54d51fd1c787">
- 컨테이너의 `/etc/downward` 디렉토리에 `path`로 정의한 파일들이 생성된다
<img width="719" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/6b572fdb-e7dc-4e8a-9d36-1c3fa0c2741b">
## 8.2 쿠버네티스 API와 통신하기

- Downward API는 유용하지만 노출할 수 있는 데이터가 제한적이다
	- 클러스터에 정의된 다른 파드에 대한 정보가 필요하면 → 쿠버네티스 API를 직접 호출하자
- `kubectl cluster-info`를 입력하면 마스터 노드에 대한 정보가 나온다.
- 쿠버네티스 API는 `https` 통신만 지원하기 때문에 인증 없이는 접근할 수 없다
	- `curl <주소> -k`를 입력해도 Forbidden과 같은 메세지를 얻게 됨
	- 이를 해결하기 위해 쿠버네티스 프록시를 사용할 수 있다.
	 - `kubeclt proxy` : 사용자 - 프록시는 HTTP로 연결, 프록시 - API 서버는 HTTPS로 연결된다.

> # [Difference between Kubernetes Objects and Resources](https://stackoverflow.com/questions/52309496/difference-between-kubernetes-objects-and-resources)
> - 클러스터에서 잡 목록을 얻기위해 `/apis/batch/v1/jobs` 리소스를 사용한다
> - 해당 리소스 안에는 여러 오브젝트들이 포함되어 있다

### 8.2.2 파드 내에서 API 서버와 통신

- API 서버 주소 찾기
	- 쿠버네티스는 서비스마다 DNS 엔트리가 있다.
	- `env`로 조회하지 않아도 `curl https://kubernetes` 와 같이 도메인 네임을 사용해서 접근할 수 있다.
- 인증방법
	- 관리자 입장에서 `kubectl`을 사용하는 것은 클라이언트 인증방식을 통해 API server에 접근하는 것이다
		- `~/.kube/config`의 내용을 요청과 함께 전달하는 것
	- `kubectl`을 사용하지 못하는 파드는 토큰 인증 방식을 사용한다
		- 파드가 생성될때 `serviceAccount admission controller`가 `Projected` volume을 생성한다(1.22v)
		- 해당 볼륨을 컨테이너의 `/var/run/secrets/kubernetes.io/serviceaccount`에 마운트한다
		- 내용을 살펴보면 `ca.crt`,`namespace`, `token` 이 있는데 여기 `token`을 API server에 해더와 함께 전달한다

**파드가 쿠버네티스와 통신하는 방법**
- 애플리케이션은 API 서버의 인증서가 인증 기관으로부터 서명되었는지 확인해야한다. 인증 기관의 인증서는 ca.cert 파일에 있다(위 과정을 통해 클라이언트가 서버를 신뢰하게 되었다.)
- token을 서버로 전달해서 클라이언트를 검증해야한다(이 과정을 통해 서버가 클라이언트를 신뢰)

# 9장. 디플로이먼트

- 기존 애플리케이션이 업데이트되었다면 파드에서 실행중인 애플리케이션을 새로운 버전으로 교체해야한다.
- 세 방법이 고려될 수 있다
	- 기존 파드를 모두 제거 후 새로운 파드 실행
	- 새로운 파드를 모두 실행한 후 기존 파드를 제거
	- 롤링 업데이트 수행
- 첫 번째 방법은 레플리케이션컨트롤러를 통해 간단하게 수행할 수 있다.

**rolling-update 명령어를 직접 입력하지 않는 이유**
- 파드 및 컨트롤러의 레이블과 레이블셀렉터 값이 관리자가 예상하지 못한 방향으로 수정된다
- `kubectl`을 이용하므로 클라이언트가 마스터를 대신하여 업데이트를 수행한다.
	- 이는 클라이언트~마스터 간 연결이 끊기면 어떠한 상태로 남아있을지 모르게 된다
- `kubectl rolling-update`는 **명령**이기 때문이다. 쿠버네티스는 선언적으로 리소스를 관리하고 알아서 해당 상태를 찾아가는 것을 중시한다

**디플로이먼트 사용하기**
- 레플리케이션컨트롤러를 사용하는 것과 유사하게 디플로이먼트를 선언하고 배포하면 된다
- pod이름의 구조는 `<Deployment-name>-<replicasetHash>-<podHash>`
- 디플로이먼트는 레플리카셋을 생성하고 생성된 레플리카셋으로 파드를 관리한다

**선언적 업데이트**
- 직접 명령어를 입력하지 않는다
- `kubectl set image ~`로 다음 버전의 이미지를 입력하자.
	- 파드 템플릿이 변경될때마다 새로운 레플리카셋이 생성된다.
	- 새로운 레플리카셋이 생성되어 롤아웃이 수행된다
- 업데이트를 수행하고 나면 `kubectl rolling-update`와는 다르게 이전 버전의 레플리카셋이 남아있다
	- 이는 새로 배포한 이미지에 문제가 있을 경우 `undo`를 위해 이전 레플리카셋을 남겨놓는다

> [!info] minReadySeconds vs initialDelaySeconds
> - 파드의 상태를 생성됨, 준비됨, 이용가능함 이라고 가정하자
> - 파드가 생성되고 요청을 받을 준비가 되었는지 확인하기 위해 readinessProbe를 사용한다
> 	- t초에 생성 + initialDelaySecond 이후에 ReadinessProbe를 실행하여 성공하면 준비됨 상태이다
> - t1초에 준비가 되었다면 t1 + minReadySecond 이후에 이용가능함 상태가 된다. 즉 요청을 받게 된다


# 10장. 스테이트풀셋

- 여러 개의 레플리카를 복제하는데 사용되는 파드 템플릿에는 볼륨클레임에 대한 참조가 있다 (예제 6.1.2를 참조하자)
- 각 인스턴스가 별도의 스토리지를 필요로 하는 분산 데이터 저장소를 실행하려면 레플리카셋을 사용할 수 없다!

전체적인 개념은 이 [영상](https://www.youtube.com/watch?v=-24U0Am7qaQ&t=929s)을 참고하자.

# 11장. 내부동작

### 11.1.2 쿠버네티스가 etcd를 사용하는 방법

- API서버만이 etcd에 접근할 수 있는 유일한 구성요소
	- 만약 다른 구성요소가 etcd에 접근한다면 다른 구성요소에도 낙관적 동시성 제어를 구현해야한다.(오메가)
	- 쿠버네티스는 API서버만이 etcd에 접근함으로써 etcd에만 낙관적 동시성 제어를 구현하면 된다
- 오브젝트들은 API서버가 비정상적으로 종료되더라도 매니페스트를  영구적으로 저장할 필요가 있다.
- 이를 위해 키-값 형태의 분산 저장소인 etcd를 활용해 저장한다
- etcd의 키는 디렉터리 형태를 띠지만 실제로 파일시스템내에 저장되는 것은 아니다.
- 값은 json 형태로 저장된다

**API 서버가 요청을 받을 때 내부적으로 발생하는 일**
- API서버는 사용자 요청을 받으면 인증모듈-인가모듈-어드미션컨트롤러를 거치게 된다
  <img width="871" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/2c2b6f9b-6de5-48f1-b773-9589ff5d1d43">
- 공식문서
  📖 인증 모듈은 클라이언트 인증서, 암호 및 일반 토큰, 부트스트랩 토큰, JWT 토큰(서비스 어카운트에 사용됨)을 포함한다.
	- `kubectl`을 사용하면 기본적으로 클라이언트 인증서를 사용하게 된다.
	- 클라이언트 인증서는 `~/.kube/config`에 저장되어 있다
	- 인증서를 사용했는지 확인하려면 `-v=7`를 추가하면 알 수 있다.

- 인증을 거치고 인가를 거치게 되고 마지막으로 Admission Control을 거친다
- Admission Control은 CUD 작업만 거치게 된다. 즉 데이터를 Read하는 요청은 어드미션 컨트롤을 거치지 않는다.
- 리소스를 여러가지 이유로 수정할 수도 있다.
- 오브젝트의 유효성을 검증한 후 etcd에 저장하고 응답을 클라이언트로 반환한다.

### 11.1.5 스케줄러 이해

- 파드를 특정 노드에서 실행하도록 결정하는 역할을 담당한다.
- 스케줄러가 직접 파드에 접근해서 실행하는 것이 아니다!
	- 스케줄러는 적절한 노드를 선택한 후 API 서버로 파드 정의를 갱신한다.
	- API 서버는 감시 메커니즘에 따라 kubelet에 갱신된 파드 정의를 전달한다
	- kubelet이 컨테이너 만들어 파드를 실행한다.
- 노드 선택
	- 파드 조건을 만족하는 노드집합을 만든다 → 각 노드마다 우선순위를 매긴다 → 우선순위에 따라 스케줄링한다
- 각 노드마다 우선순위를 매긴다
	- 두 개의 노드가 존재할 때 A 노드에서 10개의 파드가 실행중이고 B 노드에서 0개의 파드가 실행중이라면 B 노드에 스케줄링하는 것이 더 좋다.
	- 인프라를 대여해서 사용중이라면 A노드에 스케줄링하고 B노드를 반납하는 것도 좋다

### 컨트롤러

- 지금까지 컨트롤 플레인 구성요소인 etcd, API 서버, 스케줄러에 대해 알아봤다.
- etcd는 리소스에 대한 메타정보를 저장하는 분산 저장소, API 서버는 리소스를 etcd에 저장하고 통지, 스케줄러는 노드에 파드를 스케줄링한다.
- 남은 역할은 etcd에 저장된 상태대로 클러스터를 유지하는 것이다. → 컨트롤러 역할

> [!info] 공식문서 - 컨트롤러
> 쿠버네티스에서 컨트롤러는 클러스터 상태를 관찰한 다음 필요한 경우 생성 또는 변경을 요청하는 컨트롤 루프이다. 각 컨트롤러는 현재 클러스터 상태를 의도한 상태에 가깝게 한다.

### 11.1.7 Kubelet이 하는 일

- 컨트롤러는 컨트롤 플레인의 일부로 마스터 노드에서 실행되지만 Kubelet과 kube-proxy는 실제 파드를 실행하는 노드에서 실행된다.
- 첫 번째, 노드를 노드 리소스로 만들어 API 서버에 게시한다.
- 두 번째, API 서버가 노드에 파드를 스케줄링하면 설정된 런타임에 따라 컨테이너를 실행한다.

### kube-proxy

- 각 노드에 설치되어 파드간 네트워킹을 담당한다.
- 서비스의 ip와 port로 접근하면 적절한 파드로 로드밸런싱을 해준다.
- [kube-proxy](https://coffeewhale.com/k8s/network/2019/05/11/k8s-network-02/#fn:2)글을 참고하자.


**DNS 서버 동작 방식**
- 클러스터의 모든 파드는 기본적으로 클러스터의 내부 DNS서버를 사용하도록 설정돼 있다.
- DNS 서버 파드는 `kube-dns` 서비르로 노출된다.
- `kube-dns`의 IP 주소는 모든 컨테이너의 `/etc/resolv.conf`에 저장되어있다.

	<img width="636" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/2cc94174-7d6a-4f89-a020-5c1e82d70677">

- 서비스와 엔드포인트 변화를 관찰하고 모든 변화를 DNS 레코드에 갱신한다.

**인그레스 컨트롤러 동작 방식**
- 📖 DNS 애드온과 달리 몇 가지 다른 인그레스 컨트롤러 구현체가 있지만
  (아래그 구현체들의 목록이다. 공식 문서에서 확인 가능)
  <img width="710" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/81812765-3e3c-48b6-9992-56f6577a04bb">
**디플로이먼트 생성시 일어나는 일들**
- yaml 파일을 API 서버에 게시
- API 서버는 etcd에 yaml을 저장한 후 디플로이먼트를 생성
- 디플로이먼트 리소스를 감시하고 있는 여러 클라이언트 중 디플로이먼트 컨트롤러에 이벤트 전달
- 디플로이먼트 컨트롤러는 API 서버를 이용하여 레플리카셋 생성
- 레플리카셋 컨트롤러가 이를 감지하고 현재 존재하는 파드와 yaml 정의가 일치하는지 확인한다.
- 파드 생성  후 스케줄러는 이를 통보받음
- 스케줄러는 nodeName이 비어있는 파드를 감지하고 파드를 실행하기 위해 적절한 노드를 찾는다.
- 노드 할당이 완료되면 실제로 kubelet는 컨테이너를 실행한다

### 11.4.2 네트워킹 동작 방식

- 파드에 있는 `pause` 컨테이너가 네트워크 인터페이스를 관리한다.
	- 두 개의 컨테이너를 가진 파드는 결과적으로 pause 컨테이너를 포함하여 3개의 컨테이너를 실행하게 된다. (세 개 다 동일한 네임스페이스를 사용)

**동일한 노드에서 파드간 네트워킹**
- 파드 생성 시 veth 쌍이 만들어 진다. 하나는 호스트 네임스페이스에 속하게 되고 하나는 컨테이너 네임스페이스에 속해 eth0으로 변한다.
- 남아 있는 veth 들은 브리지로 연결되어 서로 통신한다.
- `eth0`은 브리지 주소 범위 안에 IP를 할당받는다. 

**다른 노드의 파드간 네트워킹**
- 다른 노드끼리 통신을 위해 브릿지 끼리 어떠한 형태로든 연결되어야 한다.
	- 오버레이 방식, 언더레이 방식 등이 있다.
- 노드의 물리적인 네트워크 인터페이스 또한 브릿지와 연결되어 있어야 한다.
- 파드의 IP 주소는 클러스터 내에서 유일해야하므로 파드 IP를 할당하는 브릿지의 주소도 서로 겹쳐서는 안된다.

# 12장. 쿠버네티스 API 서버 보안

**사용자와 그룹**
- 쿠버네티스 API 서버에 접속하는 유형은 사용자와 파드가 있다.
- 쿠버네티스는 사용자에 대한 정보를 저장하지 않는다.
- 파드마다 인증정보를 할당하기 위해 서비스어카운트를 활용한다.
	- `var/run/secrets/kubernetes.io/serviceaccount/token`에서 정보를 볼 수 있었다.

> [!info] 📖p533. 사용자 정의 토근 시크릿이 생성돼, 서비스어카운트와 연계돼 있음을 알 수 있다.
> - 쿠버네티스 v.122 이전에는 실행중인 파드에 마운트 될 수 있는 토큰 시크릿을 만들었다.
> - 쿠버네티스 최신버전에서는 TokenRequest API를 통해 직접 얻고 프로젝티드 볼륨을 사용해 마운트 된다


**RBAC 활성화 시 파드에서 서비스 목록 나열하기**
- RBAC 비활성화와 비교하기 위해 `curl localhost:8001/api/v1/namespaces/foo/services`를 실행해보자
  <img width="1283" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/75204f99-22a7-4939-9115-c3c915c34b20">
	- RBAC 비활성화 일때는 서비스어카운트를 생성만해도 다 접근이 가능했지만 지금은 그렇지 않은 것을 알 수 있다.
	- minikube에서 `kubectl api-versions` 입력 시 `.rbac.authorization.k8s.io/v1`이 보이는지 확인하면 된다.
	- 서비스어카운트 사용자 이름은 `system:serviceaccount:<namespace>:<service account name>`이다
- 서비스어카운트에 리소스에 대한 작업을 활성화 하기위해선 롤과 롤 바인딩을 사용해야한다.

**롤과 롤바인딩**
- 롤  ? 어떤 리소스에 어떤 verb를 수행할지 결정
- 롤바인딩 ? 롤과 주체를 연결해주는 리소스 (어카운트 서비스는 주체중 하나이다.)
- `kubectl create role service-reader --verb=get --verb=list --resource=services -n foo`
- `kubectl create rolebinding test --role=service-reader --serviceaccount=foo:default -n foo`
- 위 명령어를 수행한 후 다시 services를 조회해보면 제대로 조회됨을 알 수 있따.
  <img width="547" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/febca967-02ba-46ad-9b44-7ccbf33685b0">

**클러스터롤과 클러스터롤바인딩**
- URL 중에서는 네임스페이스가 필요하지 않은 리소스가 있다. (예를 들어 persistent volume, namespace, node)
- 그리고 URL 중에서는 꼭 리소스만 가리키는 것은 아니다 (예를 들어 /healthz)
- 위 유형에 대해 접근권한을 관리하기 위해 클러스터롤과 클러스터롤바인딩을 사용할 수 있다.
	- 네임스페이스가 지정되지 않은 리소스에 대해 클러스터롤 + 롤바인딩을 사용하면 효과가 없다.
	- 네임스페이스가 지정된 리소스에 대해 클러스터롤 + 롤바인딩은 효과가 있다.
- 클러스터롤 + 클러스터롤바인딩
	- 클러스터롤바인딩에 나열된 주체가 모든 네임스페이스의 리소스에 대해 권한을 갖는다.
- 클러스터롤 + 롤바인딩
	- 롤바인딩에 나열된 주체가 롤바인딩이 존재하는 네임스페이스에 속한 리소스만 볼 수 있다.
- 위 이유로 클러스터롤 + 롤바인딩 시 클러스터롤에 클러스터 수준의 리소스를 정의하면 볼 수가 없었던 것이다!..

  

---
### 메모 & 나중에 공부할 내용
- 노드 자체적으로 시크릿을 메모리에 저장한다
- RAFT 합의 알고리즘
- 워커노드의 `cbr` 네트워크 범위가 겹치면 어떤 문제가 발생할까? [관련](https://coffeewhale.com/k8s/network/2019/04/19/k8s-network-01/)
- 이것은 두 노드가 라우터 없이 같은 네트워크 스위치에 연결된 경우에만 동작한다.


