![image](https://github.com/devbelly/TIL/assets/67682840/48aacc66-5380-4e6e-89f5-058bacad0584)


# 1장. 쿠버네티스 소개

- 모놀리스 애플리케이션의 확장
	- 수직확장 : CPU, 메모리 등 하드웨어 장치의 업그레이드
		- 상한이 존재한다
	- 수평확장: 서버를 여러대 증설
		- 코드의 변경이 이루어져야한다
		- 항상 적용되는 것은 아니다.
- 마이크로서비스의 필요성이 나타나게 된다
	- 개발자가 잘 이해하고 있는 HTTP, AMQP 프로토콜을 통해 마이크로서비스간 통신을 한다
		- 최근에는 gRPC, RSocket을 활용한다
	- 하나의 구성요소가 변경되더라도 전체를 변경하지 않아도 된다
- 마이크로서비스의 단점
	- 규모가 커지면 각 구성요소들을 배포 및 관리하는 것이 어려워진다.
	- 분산된 환경에서 원격 호출하므로 디버깅 및 추적이 어렵다 → Zipkin으로 해결가능

## 1.2 컨테이너 기술 소개

- 하나의 호스트에서 각 애플리케이션마다 종속성 및 요구사항을 동시에 만족하는 것은 굉장히 어렵다
- 또한 개발자 및 운영자가 실행하는 애플리케이션 환경이 다르다
	- 마이크로서비스가 많지 않다면 VM을 통해서 운영체제 인스턴스를 제공 → 환경을 격리할 수 있다.
	- 서비스의 크기가 작아지고 갯수가 많아지면 일일이 VM을 통해서 환경을 격리한다면?
		- 하드웨어 리소스가 낭비된다
		- 일일이 관리해야하므로 인적자원이 낭비된다.
- VM 대신 리눅스 컨테이너 기술에 눈을 돌리게 되었다.
- VM와 리눅스 컨테이너
	- 차이점
		- VM은 애플리케이션이 가상화된 OS 커널에 대한 syscall을 호출 → 커널이 하이퍼바이저를 통해 물리적 CPU를 조작
		- 컨테이너는 호스트 OS와 동일한 커널에서 syscall 호출
	- 특징
		- VM은 서로 다른 커널을 보유하므로 완전한 격리를 제공한다.
		- 컨테이너는 동일한 커널을 사용하므로 보안상 완전한 격리를 제공하지 못한다
		- 실행되는 프로세스의 수가 적으면 VM, 많다면 컨테이너를 고려하자
- 리눅스 컨테이너의 기반 기술
	- namespace : 각 커널 리소스마다 가상화된 환경을 제공하여 리소스간 분리를 가능케 한다
	- cgroup : 얼마나 자원을 할당할 지 결정할 수 있다.
- 리눅스 컨테이너 기술을 쉽게 사용할 수 있도록 하는 기술이 도커
	- 애플리케이션 패키징 배포 관리 기능을 제공한다.
- 도커 컨테이너 이미지 vs VM 이미지 차이
	- 도커 컨테이너는 레이어로 이루어져 있어 다룬 이미지를 다운할 때 동일한 레이어가 있다면 나머지 레이어만 다운

#### 컨테이너에서 동일한 파일 시스템을 공유하는 방법

- 도커 이미지는 레이어로 구성되어있다.
- 레이어는 읽기 전용 레이어 + 쓰기 전용 레이어로 나뉜다. 
	- 쓰기 전용 레이어는 컨테이너 레이어이며 컨테이너가 종료될때 같이 삭제된다
- 두 레이어를 동일한 파일 시스템으로 인식한다 → 이를 Union File System이라 한다

<img width="850" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/615c0547-6e9c-4cca-9769-63ac475e74a0">
- 컨테이너는 최종적으로 `merged`를 바라보며 만약 file2에 쓰기작업이 일어나면 upperdir에 반영된다. 
- 즉, 각 컨테이너는 독립적인 파일 시스템을 유지(merged)하면서도 동일한 파일 시스템(lowerdir)을 사용한다

## 1.3 쿠버네티스 소개

### 1.3.3 쿠버네티스 클러스터 아키텍쳐 이해

- 마스터 노드 + 워커 노드로 이루어져 있다.
- 컨트롤 플레인
	- 컨트롤 플레인은 여러 구성요소로 이루어져 있다.
		- API server
			- 쿠버네티스의 프론트엔드 역할
			- 사용자와 컨트롤 플레인 구성요소와 통신한다
		- etcd(엣시디)
			- 컨트롤 플레인의 데이터 저장소
			- 설정값, 클러스터 상태를 키-값 형태로 저장한다.
		- 스케줄러
			- 애플리케이션 배포를 담당
		- 컨트롤러 매니저
			- 워커 노두 추척, 장애처리 담당
- 워커 노드
	- kubelet
	- docker engine
	- kube-proxy

#### 쿠버네티스에서 애플리케이션 실행

- 과정
	- 애플리케이션을 이미지로 패키징
	- 이미지를 레지스트리에 저장 (도커 저장소, 구글 저장소 등)
	- 쿠버네티스 API 서버에 애플리케이션 디스크립터 게시(.yml)
	- 스케줄러가 노드 정보를 확인해 노드에 컨테이너 할당 지시
	- 각 노드에 있는 kubelet이 엔진(도커)에게 이미지 저장소에서 이미지를 가져오라고 지시
	- 가져온 이미지를 컨테이너로 실행

> [!info] 컨테이너는 OS를 포함할까?
> - 기본적으로 VM과 컨테이너 차이는 OS 포함 유무
> - 하지만 컨테이너로 CentOS를 사용하는 건 뭘까?
> 	- 이는 OS 범주를 어디까지 보느냐에 달린 문제
> 	- Host OS와 커널을 공유하는 컨테이너. 커널은 OS의 핵심 기능
> 	- OS의 범주를 확장하면 커널뿐만 아니라 다른 기능들도 많음 → 이 부분을 이미지로 얻어오는 것!

#### 이동한 애플리케이션에 접근

- 쿠버네티스가 제공하는 서비스를 통해 고정 IP에 접근
- 서비스는 동적인 컨테이너 IP를 관리한다

# 2장. 도커와 쿠버네티스 첫 걸음

## 2.1 도커를 사용한 컨테이너 이미지 생성, 실행, 공유하기

- 도커가 설치되어있지 않다면 도커를 설치하자
	- 리눅스 머신에서 도커를 설치해야한다
	- 만일 리눅스가 아닌 맥, 윈도우 OS라면 리눅스 가상머신이 설치되고 그 위에 도커 데몬이 실행된다

- Dockerfile 작성
	- `app.js` 파일을 작성했으면 같은 디렉토리에 이미지 패키징을 위해 Dockerfile을 작성해보자

```Dockerfile
FROM node:7
ADD app.js /app.js
ENTRYPOINT ["node", "app.js"]
```

- From : 기본 이미지를 지정한다. 노드 애플리케이션이므로 노드 이미지를 가져와 사용한다.
- ADD : 현재 디렉토리에 있는 `app.js`를 이미지의 루트 디렉터리에 `app.js`로 추가한다
- ENTRYPOINT : 이미지를 수행했을 때 실행되어야할 명령어를 의미한다

#### 어떻게 이미지가 빌드되는지 이해하기

- `docker build -t kubia .`
	- `-t` : 빌드할 이미지의 태그 이름을 지정한다.(kubia)
	- `.` : 현재 디렉터리의 `Dockerfile`을 사용한다.
- 이미지는 여러개의 레이어로 구성되어있다.
	- 이미지가 다시 빌드될 때 레이어를 캐싱해서 사용한다.

### 컨테이너 이미지 실행

- `docker run --name kubia-container -p 8080:8080 -d kubia`
	- `-p 8080:8080` : 호스트의 8080포트를 컨테이너 8080포트와 연결한다.
		- 대부분 경우 크게 문제되진 않으나 컨테이너의 특정 포트로 접근해야하는 경우 (MySQL 3306) 문제가 된다

## 2.2 쿠버네티스 클러스터 설치

### 2.2.1 Minikube를 활용한 단일 노드 쿠버네티스 클러스터 실행하기

- Minikube는 쿠버네티스를 테스트 & 애플리케이션 개발목적으로 사용

> [!info] Mac + podman 환경에서 minikube 사용하기
> - [Using Minikube on M1 Macs](https://itnext.io/using-minikube-on-m1-macs-416da593ba0c)


**클러스터 동작 상태 확인**
- `kubectl get nodes`

**파드 이해**
- 쿠버네티스는 컨테이너를 직접 다루지 않고 다수의 컨테이너를 한거번에 다룬다 → 이를 파드라고 한다
- 가장 기본적인 단위이며 한 개 이상의 컨테이너를 포함할 수 있다.
- Pod내의 컨테이너는 IP와 Port를 공유 + 디스크 볼륨을 공유할 수 있다.

**파드 조회하기**
- 컨테이너는 쿠버네티스의 독립적인 오브젝트가 아니다
- `kubectl get pods`

**백그라운드에서 일어난 동작 이해하기**
- `docker push devbelly/kubia`
	- 도커 데몬이 다른 머신에 존재한다면 이미지를 가져오기 위해 레지스트리를 사용해야하므로 저장소에 이미지를 올려둔다
- `kubectl run kubia --image=luksa/kubia --port=8080`
	- `kubectl`에게 명령을 전달
	- `kubectl`은 마스터 노드의 REST API 서버로 명령 전달 + 클러스터에 Replication controller 생성됨
	- 스케줄러가 파드를 워커노드에 스케줄링
	- 해당 노드에 있는 `kubelet`은 이미지를 가져와서 컨테이너를 실행한다

> [!info] Replication Controller?
> - 생성된 Pod를 지속적으로 관리하기 위한 Object이다.
> - `replicas` : 지정된 숫자로 Pod를 유지한다
> - `selector` : 지정된 라벨의 Pod를 관리한다
> - `template` : 추가로 Pod를 기동할 때 사용하는 정보들을 기술한다

> [!info] 클러스터에 Replication controller이 생성 ?
> - `kubectl run kubia --image=devbelly/kubia --port=8080 --generator=run/v1`
> - 위 명령어에서 `--generator=run/v1`가 레플리케이션 컨트롤러를 생성
> - 현재는 deprecated되어 해당 flag를 제외하면 `pod/kubia create`라고 콘솔에 뜨는 것을 확인할 수 있다.
>   <img width="455" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/f31f07ac-db47-4d31-a10a-87dcbfc9cd65">

### 2.3.2 웹 애플리케이션에 접근하기

- 각 파드는 자체 IP를 가지고 있지만 클러스터 내부에 존재하므로 외부에서 접근하기 어렵다.
- 서비스를 생성해도 클러스터 내부에서 접속가능하므로 LoadBalancer 타입의 서비스를 생성해야한다
- `kubectl expose` : 서비스를 생성하여 리소스를 외부로 노출하는 역할을 한다.

**서비스 오브젝트 생성하기**
- `kubectl expose rc kubia --type=LoadBalancer --name kubia-http`
- 위에서 `replication controller`를 생성하지 않았으므로 파드를 외부로 노출하자

**파드와 컨테이너의 이해**
- 파드는 여러 개의 컨테이너를 가질 수 있다.
- 파드는 고유한 사설 IP 주소와 호스트 이름을 갖는다 → `app.js`의 `os.hostname`을 호출해도 워커노드 대신 고유한 호스트 이름이 출력됨을 확인할 수 있다. (교재에서는 kubia-4jfyf)
- 파드에 대한 자세한 정보를 알고 싶다면 `kubectl get pods -o wide` 를 사용할 수 있다.
	- 파드가 어떤 노드에 스케줄링 되어있는지 확인할 수 있다!

**레플리케이션컨트롤러의 역할 이해**
- `replicas`로 지정한 숫자만큼 pod를 띄우도록 돕는다

**서비스가 필요한 이유**
- 파드는 일시적이다
- 파드는 불완전한 노드가 삭제되면 같이 삭제될 수 있고 누군가에 의해 없이질 수도 있다.
- 레플리케이션 컨트롤러가 이를 제어해 파드를 `replicas`만큼 유지하도록 다시 생성한다.
- 이때 생성되는 사설 IP 주소는 계속해서 변경
	→ 이를 관리하기 위해 서비스가 필요하다!

# 3장. 파드 : 쿠버네티스에서 컨테이너 실행

**왜 쿠버네티스는 여러 컨테이너를 관리하는 파드라는 단위가 필요할까?**
- 선행지식) 컨테이너는 왜 단일 프로세스를 지향할까?
	- 하나의 컨테이너에서 여러 프로세스를 띄우면
		- 개별 프로세스의 추적 및 재시작을 직접 관리해야한다.
		- 표준 출력으로 인해 로깅 추적이 어렵다 → 무슨말인지…?
- 위 이유로 컨테이너를 관리하는 상위 개념인 파드를 도입한다.
	- 동시에 실행되어야 하는 컨테이너를 관리하면서도 각 컨테이너의 독립성을 유지할 수 있다.

**파드의 네임스페이스**
- 기본적으로 네임스페이스는 파드 단위로 공유되지만
- 파일 시스템에 한해서는 컨테이너 개별적으로 가지고 있다. → 쿠버네티스의 볼륨 시스템으로 파일 디렉터리를 공유할 수도 있다

**컨테이너가 동일한 IP와 포트 공간을 공유하는 방법**
- 네임스페이스는 여러가지가 존재한다. 그 중 네트워크 네임스페이스라는 것이 존재한다.
- 같은 파드끼리는 동일한 네트워크 네임스페이스를 공유한다
	→ 같은 IP와 Port 공간을 공유
- 위 이유로 동일한 파드 내 컨테이너 끼리는 `localhost:~`로 서로 통신이 가능
- 포트가 겹치지 않도록 주의해야한다!

**컨테이너 포트 지정**
- `yaml` 디스크립터를 통해서 Pod를 지정할 수 있다.
- `spec`에 정의된 `containerPort`는 다른 개발자에게 정보를 제공하기 위한 목적이다
- 공식 문서에서 그 내용을 확인할 수 있다
- 또한 포트를 명시적으로 정의하면 이름을 지정하게 편리하게 사용할 수 있다

	> List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default "0.0.0.0" address inside a container will be accessible from the network. Cannot be updated.

**kubectl create 명령으로 파드 만들기**
- `kubectl create -f kubia-manual.yaml`
- `yaml`, `json`으로 작성되었다면 위 명령어로 파드  생성 가능

**출력 로그 확인하기**
- 컨테이너화된 애플리케이션은 파일 대신 표준출력 또는 표준에러를 사용한다
- `kubectl logs <pod-name>`
- 하나의 파드에 여러 컨테이너가 존재한다면 `kubectl logs -c <container-name>`
- 파드가 삭제되면 로그도 삭제됨. 원치 않다면 중앙집중식 로그를 사용해야한다

### 3.2.5 파드에 실제 요청 보내기
- 2장에서 생성된 파드의 실제 동작을 확인하기 위해 `EXPOSE`를 통해 서비스를 생성
- 간단한 디버깅 목적을 위해서 서비스를 띄우는 대신 포트 포워딩을 사용하자
	<img width="389" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/1b6ef23b-43fa-4efe-8001-f106dc94dff8">

## 3.4 레이블 셀럭터를 이용한 파드 부분 집합 나열

- 쿠버네티스의 실제 운영환경은 여러개의 파드(수십~수백)로 이루어져있다.
- 여러 파드를 관리하기 위해 `키-값`으로 구성된 레이블을 사용
- 레이블을 추가하는 방법
	- `yaml`에 작성
	- 명령어를 통해 추가, 수정(수정시 `--overwrite` 플래그가 필요하다)
- 레이블은 파드만이 가질 수 있는 것이 아니다! 쿠버네티스 오브젝트라면 가질 수 있다.
- 만약 특정 노드에 파드를 스케줄링 해야한다면? (인프라에 대한 의존도가 생기긴 한다)
	- 노드에 `gpu=true` 레이블을 추가한 후
	- 파드에 `nodeSelector` 설정에 `gpu: "true"` 를 적는다.

