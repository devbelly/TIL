![image](https://github.com/devbelly/TIL/assets/67682840/48aacc66-5380-4e6e-89f5-058bacad0584)


# 1장. 쿠버네티스 소개

- 모놀리스 애플리케이션의 확장
	- 수직확장 : CPU, 메모리 등 하드웨어 장치의 업그레이드
		- 상한이 존재한다
	- 수평확장: 서버를 여러대 증설
		- 코드의 변경이 이루어져야한다
		- 항상 적용되는 것은 아니다.
- 마이크로서비스의 필요성이 나타나게 된다
	- 개발자가 잘 이해하고 있는 HTTP, AMQP 프로토콜을 통해 마이크로서비스간 통신을 한다
		- 최근에는 gRPC, RSocket을 활용한다
	- 하나의 구성요소가 변경되더라도 전체를 변경하지 않아도 된다
- 마이크로서비스의 단점
	- 규모가 커지면 각 구성요소들을 배포 및 관리하는 것이 어려워진다.
	- 분산된 환경에서 원격 호출하므로 디버깅 및 추적이 어렵다 → Zipkin으로 해결가능

## 1.2 컨테이너 기술 소개

- 하나의 호스트에서 각 애플리케이션마다 종속성 및 요구사항을 동시에 만족하는 것은 굉장히 어렵다
- 또한 개발자 및 운영자가 실행하는 애플리케이션 환경이 다르다
	- 마이크로서비스가 많지 않다면 VM을 통해서 운영체제 인스턴스를 제공 → 환경을 격리할 수 있다.
	- 서비스의 크기가 작아지고 갯수가 많아지면 일일이 VM을 통해서 환경을 격리한다면?
		- 하드웨어 리소스가 낭비된다
		- 일일이 관리해야하므로 인적자원이 낭비된다.
- VM 대신 리눅스 컨테이너 기술에 눈을 돌리게 되었다.
- VM와 리눅스 컨테이너
	- 차이점
		- VM은 애플리케이션이 가상화된 OS 커널에 대한 syscall을 호출 → 커널이 하이퍼바이저를 통해 물리적 CPU를 조작
		- 컨테이너는 호스트 OS와 동일한 커널에서 syscall 호출
	- 특징
		- VM은 서로 다른 커널을 보유하므로 완전한 격리를 제공한다.
		- 컨테이너는 동일한 커널을 사용하므로 보안상 완전한 격리를 제공하지 못한다
		- 실행되는 프로세스의 수가 적으면 VM, 많다면 컨테이너를 고려하자
- 리눅스 컨테이너의 기반 기술
	- namespace : 각 커널 리소스마다 가상화된 환경을 제공하여 리소스간 분리를 가능케 한다
	- cgroup : 얼마나 자원을 할당할 지 결정할 수 있다.
- 리눅스 컨테이너 기술을 쉽게 사용할 수 있도록 하는 기술이 도커
	- 애플리케이션 패키징 배포 관리 기능을 제공한다.
- 도커 컨테이너 이미지 vs VM 이미지 차이
	- 도커 컨테이너는 레이어로 이루어져 있어 다룬 이미지를 다운할 때 동일한 레이어가 있다면 나머지 레이어만 다운

#### 컨테이너에서 동일한 파일 시스템을 공유하는 방법

- 도커 이미지는 레이어로 구성되어있다.
- 레이어는 읽기 전용 레이어 + 쓰기 전용 레이어로 나뉜다. 
	- 쓰기 전용 레이어는 컨테이너 레이어이며 컨테이너가 종료될때 같이 삭제된다
- 두 레이어를 동일한 파일 시스템으로 인식한다 → 이를 Union File System이라 한다

<img width="850" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/615c0547-6e9c-4cca-9769-63ac475e74a0">
- 컨테이너는 최종적으로 `merged`를 바라보며 만약 file2에 쓰기작업이 일어나면 upperdir에 반영된다. 
- 즉, 각 컨테이너는 독립적인 파일 시스템을 유지(merged)하면서도 동일한 파일 시스템(lowerdir)을 사용한다

## 1.3 쿠버네티스 소개

### 1.3.3 쿠버네티스 클러스터 아키텍쳐 이해

- 마스터 노드 + 워커 노드로 이루어져 있다.
- 컨트롤 플레인
	- 컨트롤 플레인은 여러 구성요소로 이루어져 있다.
		- API server
			- 쿠버네티스의 프론트엔드 역할
			- 사용자와 컨트롤 플레인 구성요소와 통신한다
		- etcd(엣시디)
			- 컨트롤 플레인의 데이터 저장소
			- 설정값, 클러스터 상태를 키-값 형태로 저장한다.
		- 스케줄러
			- 애플리케이션 배포를 담당
		- 컨트롤러 매니저
			- 워커 노두 추척, 장애처리 담당
- 워커 노드
	- kubelet
	- docker engine
	- kube-proxy

#### 쿠버네티스에서 애플리케이션 실행

- 과정
	- 애플리케이션을 이미지로 패키징
	- 이미지를 레지스트리에 저장 (도커 저장소, 구글 저장소 등)
	- 쿠버네티스 API 서버에 애플리케이션 디스크립터 게시(.yml)
	- 스케줄러가 노드 정보를 확인해 노드에 컨테이너 할당 지시
	- 각 노드에 있는 kubelet이 엔진(도커)에게 이미지 저장소에서 이미지를 가져오라고 지시
	- 가져온 이미지를 컨테이너로 실행

> [!info] 컨테이너는 OS를 포함할까?
> - 기본적으로 VM과 컨테이너 차이는 OS 포함 유무
> - 하지만 컨테이너로 CentOS를 사용하는 건 뭘까?
> 	- 이는 OS 범주를 어디까지 보느냐에 달린 문제
> 	- Host OS와 커널을 공유하는 컨테이너. 커널은 OS의 핵심 기능
> 	- OS의 범주를 확장하면 커널뿐만 아니라 다른 기능들도 많음 → 이 부분을 이미지로 얻어오는 것!

#### 이동한 애플리케이션에 접근

- 쿠버네티스가 제공하는 서비스를 통해 고정 IP에 접근
- 서비스는 동적인 컨테이너 IP를 관리한다

# 2장. 도커와 쿠버네티스 첫 걸음

## 2.1 도커를 사용한 컨테이너 이미지 생성, 실행, 공유하기

- 도커가 설치되어있지 않다면 도커를 설치하자
	- 리눅스 머신에서 도커를 설치해야한다
	- 만일 리눅스가 아닌 맥, 윈도우 OS라면 리눅스 가상머신이 설치되고 그 위에 도커 데몬이 실행된다

- Dockerfile 작성
	- `app.js` 파일을 작성했으면 같은 디렉토리에 이미지 패키징을 위해 Dockerfile을 작성해보자

```Dockerfile
FROM node:7
ADD app.js /app.js
ENTRYPOINT ["node", "app.js"]
```

- From : 기본 이미지를 지정한다. 노드 애플리케이션이므로 노드 이미지를 가져와 사용한다.
- ADD : 현재 디렉토리에 있는 `app.js`를 이미지의 루트 디렉터리에 `app.js`로 추가한다
- ENTRYPOINT : 이미지를 수행했을 때 실행되어야할 명령어를 의미한다

#### 어떻게 이미지가 빌드되는지 이해하기

- `docker build -t kubia .`
	- `-t` : 빌드할 이미지의 태그 이름을 지정한다.(kubia)
	- `.` : 현재 디렉터리의 `Dockerfile`을 사용한다.
- 이미지는 여러개의 레이어로 구성되어있다.
	- 이미지가 다시 빌드될 때 레이어를 캐싱해서 사용한다.

### 컨테이너 이미지 실행

- `docker run --name kubia-container -p 8080:8080 -d kubia`
	- `-p 8080:8080` : 호스트의 8080포트를 컨테이너 8080포트와 연결한다.
		- 대부분 경우 크게 문제되진 않으나 컨테이너의 특정 포트로 접근해야하는 경우 (MySQL 3306) 문제가 된다

## 2.2 쿠버네티스 클러스터 설치

### 2.2.1 Minikube를 활용한 단일 노드 쿠버네티스 클러스터 실행하기

- Minikube는 쿠버네티스를 테스트 & 애플리케이션 개발목적으로 사용

> [!info] Mac + podman 환경에서 minikube 사용하기
> - [Using Minikube on M1 Macs](https://itnext.io/using-minikube-on-m1-macs-416da593ba0c)


**클러스터 동작 상태 확인**
- `kubectl get nodes`

**파드 이해**
- 쿠버네티스는 컨테이너를 직접 다루지 않고 다수의 컨테이너를 한거번에 다룬다 → 이를 파드라고 한다
- 가장 기본적인 단위이며 한 개 이상의 컨테이너를 포함할 수 있다.
- Pod내의 컨테이너는 IP와 Port를 공유 + 디스크 볼륨을 공유할 수 있다.

**파드 조회하기**
- 컨테이너는 쿠버네티스의 독립적인 오브젝트가 아니다
- `kubectl get pods`

**백그라운드에서 일어난 동작 이해하기**
- `docker push devbelly/kubia`
	- 도커 데몬이 다른 머신에 존재한다면 이미지를 가져오기 위해 레지스트리를 사용해야하므로 저장소에 이미지를 올려둔다
- `kubectl run kubia --image=luksa/kubia --port=8080`
	- `kubectl`에게 명령을 전달
	- `kubectl`은 마스터 노드의 REST API 서버로 명령 전달 + 클러스터에 Replication controller 생성됨
	- 스케줄러가 파드를 워커노드에 스케줄링
	- 해당 노드에 있는 `kubelet`은 이미지를 가져와서 컨테이너를 실행한다

> [!info] Replication Controller?
> - 생성된 Pod를 지속적으로 관리하기 위한 Object이다.
> - `replicas` : 지정된 숫자로 Pod를 유지한다
> - `selector` : 지정된 라벨의 Pod를 관리한다
> - `template` : 추가로 Pod를 기동할 때 사용하는 정보들을 기술한다

> [!info] 클러스터에 Replication controller이 생성 ?
> - `kubectl run kubia --image=devbelly/kubia --port=8080 --generator=run/v1`
> - 위 명령어에서 `--generator=run/v1`가 레플리케이션 컨트롤러를 생성
> - 현재는 deprecated되어 해당 flag를 제외하면 `pod/kubia create`라고 콘솔에 뜨는 것을 확인할 수 있다.
>   <img width="455" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/f31f07ac-db47-4d31-a10a-87dcbfc9cd65">

### 2.3.2 웹 애플리케이션에 접근하기

- 각 파드는 자체 IP를 가지고 있지만 클러스터 내부에 존재하므로 외부에서 접근하기 어렵다.
- 서비스를 생성해도 클러스터 내부에서 접속가능하므로 LoadBalancer 타입의 서비스를 생성해야한다
- `kubectl expose` : 서비스를 생성하여 리소스를 외부로 노출하는 역할을 한다.

**서비스 오브젝트 생성하기**
- `kubectl expose rc kubia --type=LoadBalancer --name kubia-http`
- 위에서 `replication controller`를 생성하지 않았으므로 파드를 외부로 노출하자

**파드와 컨테이너의 이해**
- 파드는 여러 개의 컨테이너를 가질 수 있다.
- 파드는 고유한 사설 IP 주소와 호스트 이름을 갖는다 → `app.js`의 `os.hostname`을 호출해도 워커노드 대신 고유한 호스트 이름이 출력됨을 확인할 수 있다. (교재에서는 kubia-4jfyf)
- 파드에 대한 자세한 정보를 알고 싶다면 `kubectl get pods -o wide` 를 사용할 수 있다.
	- 파드가 어떤 노드에 스케줄링 되어있는지 확인할 수 있다!

**레플리케이션컨트롤러의 역할 이해**
- `replicas`로 지정한 숫자만큼 pod를 띄우도록 돕는다

**서비스가 필요한 이유**
- 파드는 일시적이다
- 파드는 불완전한 노드가 삭제되면 같이 삭제될 수 있고 누군가에 의해 없이질 수도 있다.
- 레플리케이션 컨트롤러가 이를 제어해 파드를 `replicas`만큼 유지하도록 다시 생성한다.
- 이때 생성되는 사설 IP 주소는 계속해서 변경
	→ 이를 관리하기 위해 서비스가 필요하다!

# 3장. 파드 : 쿠버네티스에서 컨테이너 실행

**왜 쿠버네티스는 여러 컨테이너를 관리하는 파드라는 단위가 필요할까?**
- 선행지식) 컨테이너는 왜 단일 프로세스를 지향할까?
	- 하나의 컨테이너에서 여러 프로세스를 띄우면
		- 개별 프로세스의 추적 및 재시작을 직접 관리해야한다.
		- 표준 출력으로 인해 로깅 추적이 어렵다 → 무슨말인지…?
- 위 이유로 컨테이너를 관리하는 상위 개념인 파드를 도입한다.
	- 동시에 실행되어야 하는 컨테이너를 관리하면서도 각 컨테이너의 독립성을 유지할 수 있다.

**파드의 네임스페이스**
- 기본적으로 네임스페이스는 파드 단위로 공유되지만
- 파일 시스템에 한해서는 컨테이너 개별적으로 가지고 있다. → 쿠버네티스의 볼륨 시스템으로 파일 디렉터리를 공유할 수도 있다

**컨테이너가 동일한 IP와 포트 공간을 공유하는 방법**
- 네임스페이스는 여러가지가 존재한다. 그 중 네트워크 네임스페이스라는 것이 존재한다.
- 같은 파드끼리는 동일한 네트워크 네임스페이스를 공유한다
	→ 같은 IP와 Port 공간을 공유
- 위 이유로 동일한 파드 내 컨테이너 끼리는 `localhost:~`로 서로 통신이 가능
- 포트가 겹치지 않도록 주의해야한다!

**컨테이너 포트 지정**
- `yaml` 디스크립터를 통해서 Pod를 지정할 수 있다.
- `spec`에 정의된 `containerPort`는 다른 개발자에게 정보를 제공하기 위한 목적이다
- 공식 문서에서 그 내용을 확인할 수 있다
- 또한 포트를 명시적으로 정의하면 이름을 지정하게 편리하게 사용할 수 있다

	> List of ports to expose from the container. Exposing a port here gives the system additional information about the network connections a container uses, but is primarily informational. Not specifying a port here DOES NOT prevent that port from being exposed. Any port which is listening on the default "0.0.0.0" address inside a container will be accessible from the network. Cannot be updated.

**kubectl create 명령으로 파드 만들기**
- `kubectl create -f kubia-manual.yaml`
- `yaml`, `json`으로 작성되었다면 위 명령어로 파드  생성 가능

**출력 로그 확인하기**
- 컨테이너화된 애플리케이션은 파일 대신 표준출력 또는 표준에러를 사용한다
- `kubectl logs <pod-name>`
- 하나의 파드에 여러 컨테이너가 존재한다면 `kubectl logs -c <container-name>`
- 파드가 삭제되면 로그도 삭제됨. 원치 않다면 중앙집중식 로그를 사용해야한다

### 3.2.5 파드에 실제 요청 보내기
- 2장에서 생성된 파드의 실제 동작을 확인하기 위해 `EXPOSE`를 통해 서비스를 생성
- 간단한 디버깅 목적을 위해서 서비스를 띄우는 대신 포트 포워딩을 사용하자
	<img width="389" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/1b6ef23b-43fa-4efe-8001-f106dc94dff8">

## 3.4 레이블 셀럭터를 이용한 파드 부분 집합 나열

- 쿠버네티스의 실제 운영환경은 여러개의 파드(수십~수백)로 이루어져있다.
- 여러 파드를 관리하기 위해 `키-값`으로 구성된 레이블을 사용
- 레이블을 추가하는 방법
	- `yaml`에 작성
	- 명령어를 통해 추가, 수정(수정시 `--overwrite` 플래그가 필요하다)
- 레이블은 파드만이 가질 수 있는 것이 아니다! 쿠버네티스 오브젝트라면 가질 수 있다.
- 만약 특정 노드에 파드를 스케줄링 해야한다면? (인프라에 대한 의존도가 생기긴 한다)
	- 노드에 `gpu=true` 레이블을 추가한 후
	- 파드에 `nodeSelector` 설정에 `gpu: "true"` 를 적는다.

## 3.7 네임스페이스를 사용한 리소스 그룹화

- 레이블을 이용해 다른 오브젝트를 묶을 수 있지만 각  오브젝트는 여러 여러 레이블을 갖기 때문에 교집합이 되는 오브젝트가 생긴다
- 겹치지 않는 리소스로 구분하기 위해서는 네임스페이스 사용
	- 이는 리눅스 네임스페이스와는 다른 개념

> [!faq] 쿠버네티스 네임스페이스는 오브젝트 이름의 범위를 제공한다
> - 리소스 이름은 네임스페이스 내에서 유일
> - 네임스페이스를 통틀어서 유일할 필요는 없다.
> - 여러 개발자들이 자신의 네임스페이스에서 작업한 것이 다른 사람의 네임스페이스에 영향을 끼치지 않도록 한다


- `kubectl get ns`으로 가지고 있는 네임스페이스 목록을 출력할 수 있다
- `kubectl get po`는 ns를 지정하지 않았기 때문에 기본적으로 `default namespace`에 작업을 한다

> [!faq] 오브젝트와 리소스
> - 오브젝트 : 구체적인 객체를 가리킨다. 
> - 리소스 : API endpoint를 가리킨다. 대부분 오브젝트를 가리키나(GET) 간혹 동작을 가리키기도 한다(POST)
> - 리소스와 오브젝트는 클래스와 객체의 관계와 일치한다.
> 	- `kubectl get po`했을때 결과가 `apple`, `banana` 라면 
> 	- `apple`, `banana`는 오브젝트
> 	- `po`는 리소스


- 네임스페이스에 리소스를 만들기 위해서는 `namespace: custom-namespace`를 지정하거나 `kubectl create`를 사용한다

# 4장. 레플리케이션과 그 밖의 컨트롤러: 관리되는 파드 배포

## 4.1 파드를 안정적으로 유지하기

- 컨테이너 리스트를 쿠버네티스에 전달하면 쿠버네티스는 파드가 사라지지 않는 한 컨테이너를 계속 관리한다
- 만약 컨테이너의 주 프로세스가 예기치 못한 이유로 강제종료되면 쿠버네티스는 컨테이너를 다시 실행한다
- 강제종료가 된 상황이 아니라 컨테이너의 프로세스가 무한루프 또는 데드락 같이 응답을 제대로 못 보내는 상황이면 어떡할까?
	- 이러한 상황도 쿠버네티스가 관리하기 위해서 애플리케이션 내부적으로 상태를 추적하는 것 뿐만 아니라 애플리케이션 외부에서도 컨테이너 상태를 계속 확인할 수 있어야 한다
	- 이를 위해서 liveness probe를 사용!

**liveness probe의 세가지 매커니즘**
- Http Get
	- 지정된 IP, port, 주소를 기반으로 http get 메세지를 날려서 응답하는 코드에 따라 컨테이너를 재실행한다
- TCP socekt
	- 지정된 port를 기반으로 TCP 연결을 시도한다
- Exec 프로브
	- 컨테이너에 임의의 명령을 실행한 후 리턴되는 값을 확인한다

<img width="445" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/0ff1d964-d9a3-4478-a41c-eaa7985da4ce">
- 일정시간마다 Http Get요청을 보내고 실패하여 RESTARTS가 1로 증가한 것을 확인할 수 있다.
- `kubectl describe po liveness-probe`를 통해 종료된 이유를 확인할 수 있다.
	- 종료코드는 137로 128+x를 의미
	- 외부 신호에 의해 컨테이너가 종료되었다는 것을 확인. x는 9 이므로 강제종료되었다는 것을 알 수 있다.
- 컨테이너가 비정상적으로 작동 시 노드의 `kubelet`이 컨테이너를 재실행할 책임을 지며
	- 노드가 비정상적으로 종료되면 마스터 노드의 `control-plane`이 파드를 재실행하게 된다

## 4.2 레플리케이션 컨트롤러

- 파드의 갯수가 일정하게 유지되도록 하는 리소스
	- 노드가 비정상적으로 종료되어 파드가 사라지거나
	- 클러스터에서 노드가 사라졌을때
- 레이블 셀렉터와 매치되는 파드를 찾아 항상 일정수를 유지하고 있는지 모니터링한다
- 구성요소
	- 레이블 셀렉터
	- 템플릿
	- 레플리카
- 레이블 셀렉터와 템플릿의 변경은 현재 레플리케이션컨트롤러가 관리하고 있는 파드에 영향을 끼치지 않는다.
- 레이블 셀렉터와 템플릿의 파드 레이블은 완전히 일치해야한다
	- 일치하지 않으면 파드를 무한정 생성, 이를 방지하기 위해 미리 검사하기는 한다
	- 레이블 셀렉터를 지정하지 않고 템플릿의 파드 레이블을 자동으로 찾게끔 작성하자
		- `yaml`를 가볍게 유지할 수 있고 실수를 줄일 수 있다.


	<img width="473" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/3cb1c9fa-e55a-45b0-b39c-ba6abe804fec">
- 생성 직후

<img width="457" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/ba21bb4d-aae9-42d9-8c1e-45ea16597204">
- 임의로 파드를 삭제하더라도 ReplicationController설정에 따라 자동으로 재시작함을 알 수 있다.

> [!info] 파드는 레플리케이션컨트롤러와 묶여있지 않다.
> - 레플리케이션컨트롤러는 레이블 셀렉터로 설정한 파드만 관리한다
> - 파드는 `metadata.ownerReference` 필드를 참조하여 레플리케이션컨트롤러를 쉽게 찾을 수 있다

> [!faq] 레플리케이션컨트롤러로 생성한 파드의 레이블을 변경하면 수동으로 생성한 파드와 동일해지는데 `metadata.ownerReference`는 어떻게 변할까?

`레이블 변경 전`
<img width="1103" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/d6da67ca-1a2a-432c-8f25-6706687bd402">
`레이블 변경 후`
<img width="641" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/f6d610cd-0ca4-4b15-9631-2c24c776ed49">
- 더이상 정보가 표시되지 않는다

- 레플리케이션컨트롤러가 삭제되면 관리되는 파드도 기본적으로 삭제되는 정책을 가진다.
	- 관리되는 파드를 남기고 싶을 수도 있다.(예를 들어 레플리케이션컨트롤러를 레플리케이션셋으로 변경하는 경우)
	- `--cascade=false`로 설정가능하다

### 4.3.2 레플리카셋 정의하기

- 레플리카셋은 레플리케이션컨트롤러와 거의 유사하나 레이블을 선택할 때 다양한 표현식을 사용할 수 있다
- 레플리케이션 컨트롤러에서 버린 파드를 주워서 사용해보자
- `kubectl describe rs`를 하면 `Event`가 없는 것을 알 수 있다. → 주워서 사용중

## 4.4 데몬셋

- 지금까지 살펴본 레플리케이션컨트롤러 & 레플리카셋은 클러스터에 지정한 파드만큼 무작위로 배포
- 하지만 노드마다 하나의 파드만이 실행되기를 원할수도 있다
	- 시스템 관련 파드들이 이에 해당한다. 모니터링
- 이럴때 데몬셋을 활용할 수 있고 모든 노드마다 파드를 실행하기를 원하지 않는다면 `nodeSelector`를 통해 특정 노드에서만 파드를 실행할 수 있다
- 데몬셋의 개념 때문에 `replicas` 개념이 없다

## 4.5 잡

- 지금까지 살펴본 레플리케이션컨트롤러 & 레플리카셋& 데몬셋은 항상 컨테이너가 실행되어야한다는 공통점이 있다.
- 하지만 컨테이너가 태스크를 완료했다면 컨테이너를 다시 시작하지 않고 종료되기를 원한다면 어떡할까? → 잡 리소스를 사용할 수 있다
- 노드가 비정상적으로 종료되면 파드는 다른 노드에 자동으로 스케줄링된다
- 프로세스가 비정상적으로 종료되면 잡에서 다시 시작할지에 대한 유무를 선택할 수 있다.
- `kubectl get job`을 해보면 숫자로 표시된다
	- 이는 `completions`, `parallelism` 옵션이 더 있음을 예상할 수 있다.

## 4.6 크론잡

- 잡을 주기적으로 실행하려면 크론잡 리소스를 사용할 수 있다.
- 쿠버네티스는 설정된 시간에 크론잡 오브젝트에 설정한 잡 템플릿에 따라 생성한다
- 잡 템플릿은 설정한 파드 템플릿에 따라 파드를 생성하고 실행한 후 종료된다

# 5장. 서비스

- 쿠버네티스는 파드가 계속 생성되고 삭제되는 일이 잦다.
- 위 이유로 파드의 IP주소는 계속 변경되어 클라이언트에서는 고정된 IP에 접근하기 어렵다
- 위 문제를 해결하기 위해 서비스 리소스를 이용할 수 있다
- 이전에 `kubectl expose`를 통해 간단하게 서비스를 만들었지만 `yaml` 로 작성하는 방법에 대해 알아보자

```yaml
apiVersion: v1

kind: Service

metadata:

name: kubia

spec:

ports:

- port: 80

targetPort: 8080

selector:

app: kubia
```

	- 서비스의  포트는 80이다
	- 서비스가 포워드할 컨테이너 포트는 8080이다 -> 이미지 생성시 사용한 포트
	- `app: kubia`를 가지고 있는 파드를 서비스로 묶는다

- `kubectl get svc`를 통해 현재 실행중인 서비스를 확인할 수 있다.
	- 기본 목적은 파드의 그룹화지만 주로 파드를 외부 노출하기 위해 사용한다.
	- 현재는 클러스터 내부 IP가 할당된 것을 알 수 있다

<img width="476" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/92f4f35f-adbb-4e2c-9a0c-98a0ed13d6f3">
- 내부 컨테이너에 접속해서 서비스의 내부 IP에 `curl` 요청을 보내고 있다
- 서비스 프록시가 요청을 랜덤한 파드로 보내기 때문에 실행마다 결과값이 변하고 있다
- 클라이언트 IP마다 처리할 파드를 고정하기 위해서는 `sessionAffinity` 옵션을 추가하면 된다

**이름이 지정된 포트 사용**
- 서비스의 타켓포트를 번호로 지정하는 대신 이름을 사용할 수 있다
- 파드 생성시 컨테이너포트에 이름을 지정한 후 서비스에서 이 이름을 사용할 수 있다
	- 이전에 파드에서 컨테이너포트를 지정한 것은 개발자를 위한 것이라고 했다
	- 뿐만 아니라 이름을 지정하여 편리하게 사용할 수 있다고 했는데 지금 이 부분이다

**환경변수를 통한 서비스 검색**
- 생성한 서비스를 클라이언트 파드에게 일일이 전달하는 대신 검색할 수 있는 기능을 제공한다
- 파드가 생성되면 해당 시점에 존재하는 서비스에 대한 환경변수를 파드에 저장하게 된다
	- 즉 파드생성 → 서비스생성시 서비스를 제대로 찾지 못한다
- `kubectl exec <pod> env`

 **DNS를 통한 서비스 검색**
 - `kube-system` 네임스페이스에는 `kube-dns`이름의 서비스가 있다
	<img width="797" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/d0f55608-c88f-4521-99b4-96a596d6b10f">
- 모든 파드에서 이뤄지는 dns 질의는 `kube-dns`를 사용하도록 설정되어있다
	- 이는 리눅스 시스템의 `/etc/resolve.conf`를 확인해보면 알 수 있다
	- <img width="611" alt="image" src="https://github.com/devbelly/TIL/assets/67682840/1dbd0bab-45ca-4ec3-8fe2-4a8185246884">

### 5.2.1 서비스 엔드포인트
- 서비스는 파드에 직접 연결되지 않는다
- 서비스와 파드 사이에 엔드포인드가 있다.(엔드포인트도 리소스이다)
- 파드 셀렉터는 엔드포인트 목록을 만드는데 사용된다

```yaml
apiVersion: v1
kind: Service
metadata:
	name: external-service
spec:
	ports:
	- port: 80
```
	 
### 5.2.3 외부 서비스를 위한 별칭 생성

- 엔드포인트를 직접 구성하는 대신 FQDN을 통해 외부 서비스를 참조할 수 있다.
- `externalName.` 클러스터 내부 파드가 외부 서비스에 쉽게 접근하기 위해 사용한다
- CNAME DNS record를 생성하여 coreDNS에 해당 정보를 저장한다

## 5.3 외부 클라이언트에 서비스 노출

- 내부→외부로 접근하는 `externalName` 또는 수동으로 엔드포인트 구성
- 외부→내부로 접근하려면 어떻게 해야할까?

### 5.3.1 노트포트 서비스 사용

- 기본방식인 클러스터의 IP 접근 + (노드 IP+노트 Port)로 추가접근이 가능
- 접근
	- `10.111.254.223:80`
	- `<첫 번째 노드의 IP>:30123`
	- `<두 번째 노드의 IP>:30123`

### 5.3.2 외부 로드밸런서로 서비스 노출

- 클라이언트는 접근하는 노드에 상관없이 서비스에 접근할 수 있어야한다
	- 하지만 노트포트로 접근하면 접근하는 노드의 상태에 따라 서비스에 접근하지 못하는 경우도 발생한다
- 로드밸랜서를 사용해서 해결하자
- `yaml` 파일 작성 시 `nodePort`를 지정할 수 있지만 작성하지 않는다 → 쿠버네티스가 직접 선택하도록 한다

### 5.3.3 외부 연결 특성 이해

- 불필요한 hop
	- 노트포트로 노드 접근 → 서비스 접근 → 파드 접근
	- 만약 노드에 처음 접근했을 때 파드가 실행된다면 불필요한 홉이 발생한다
	- `externalTrafficPolicy`
		- 외부 트래픽에 대한 정책을 결정한다.
			- Local 사용 시 불필요한 홉을 막을 수 있으나 트래픽 불균형을 가져온다

- 클라이언트 IP 변경
	- 노트포트 사용 시 SNAT 때문에 클라이언트 IP가 변경된다
	- `externalTrafficPolicy`를 Local로 하면 변경되는 것을 막을 수 있다
	- [여기](https://kubernetes.io/ko/docs/tutorials/services/source-ip/) 를 참고하자..

## 5.4 인그레스

- 서비스는 L4 계층에서 작동. L7 계층에서 로드밸런싱을 지원하기 위해 인그레스를 사용할 수 있다
	- 이는 도메인 이름 마다 또는 경로마다 로드밸런싱을 지원할 수 있따
- 사용하기 위해서는 인그레스 컨트롤러를 설치해야한다
- TLS도 지원할 수 있다. TLS termination 기능 또한 지원
	- TLS termination을 통해 성능 향상을 기대할 수 있다

## 5.5 레디니스 프로브

- 새로운 파드가 추가될 때 온전한 요청을 수신하기까지 시간이 필요할 수도 있다 → 레디니스 프로브를 통해 이를 확인할 수 있다.
- 라이브니스 프로브와 마찬가지로 레디니스 프로브도 세가지 유형이 존재한다
	- exec
	- Http Get
	- TCP 소켓
- 주기적으로 프로브를 호출해 상태를 점검한다. 이때 만약 준비되지 않았다면 서비스의 엔드포인트에서 제거되었다가 준비되면 다시 엔드포인트에 추가된다
	- 서비스의 엔드포인트는 `kubectl describe svc <service-name>` 에서 확인한다

> [!faq] p252. 레디니스 프로브에 파드의 종료 코드를 포함하지 마라

# 6장. 볼륨

- 파드는 내부에 프로세스가 실행되고 CPU, RAM, 네트워크 인터페이스 등의 리소스를 공유하는 논리적인 호스트와 유사하다 → 파드 내부의 컨테이너끼리는 `localhost:port`로 통신하기 때문이다
- 프로세스가 디스크를 공유하듯 컨테이너끼리 디스크를 공유할 수 있을까? 
	- 파드 내부의 컨테이너는 독립적인 파일 시스템을 가진다
	- UFS를 사용한다. 책 설명대로 파일 시스템은 컨테이너 이미지에서 제공!
- 📖 새로 시작한 컨테이너는 이전에 실행했던 컨테이너에 쓰여진 파일 시스템의 어떤 것도 볼 수 없다
	- 컨테이너에서 사용하는 파일 시스템은 UFS
	- UFS는 `image layer` + `container layer`가 합쳐진 형태
	- 컨테이너가 생성되며 생기는 `container layer`는 컨테이너 종료시 사라지므로 이전에 실행했던 내용을 볼 수 없는 것!
